{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents\n",
    "\n",
    "1. Linear regression\n",
    "2. Linear regression with sklearn\n",
    "3. Evaluation metrics\n",
    "4. Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Regression\n",
    "\n",
    "Regression, in the context of statistics and machine learning, refers to a set of techniques used to model the relationship between a dependent variable (target) and one or more independent variables (features).\n",
    "\n",
    "The goal of regression analysis is to understand the nature of the relationship and make predictions based on the given data. It is a supervised learning approach, where the algorithm learns from labeled training data to predict the outcome for new, unseen data.\n",
    "\n",
    "\n",
    "Regression problems can be broadly categorized into two types:\n",
    "\n",
    "* **Linear Regression**: Assumes a linear relationship between the independent variables and the dependent variable.\n",
    "* **Nonlinear Regression**: Assumes a nonlinear relationship between the variables. Example : Polynomial regression\n",
    "\n",
    "In both cases, the term “regression” is used because the goal is to model or predict a continuous outcome, as opposed to classification, where the goal is to predict a categorical outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In **Simple Linear Regression**, the term “best fit line” usually refers to a straight line in the case of simple linear regression (one independent variable).\n",
    "\n",
    "**Notation for the Lineal Regression model**.\n",
    "\n",
    "\n",
    "We will have a parameter $\\textbf{y}$ that depends linearly on several covariates $\\textbf{x}_i$:\n",
    "\n",
    "\n",
    "$$ \\textbf{y}  =  a_1 \\textbf{x}_1  + \\dots + a_m \\textbf{x}_{m} $$\n",
    "\n",
    "The $a_i$ terms will be the *parameters* of the model or *coefficients*.\n",
    "\n",
    "If we write it in matrix form:\n",
    "\n",
    "$$ \\textbf{y}  = X \\textbf{w}$$\n",
    "\n",
    "Where $$ \\textbf{y} = \\left( \\begin{array}{c} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{array} \\right), \n",
    " X = \\left( \\begin{array}{c} x_{11}  \\dots x_{1m} \\\\ x_{21}  \\dots x_{2m}\\\\ \\vdots \\\\ x_{n1}  \\dots x_{nm} \\end{array} \\right),\n",
    " \\textbf{w} = \\left( \\begin{array}{c} a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_m \\end{array} \\right) $$\n",
    " \n",
    "In a **simple linear regression** model that only depends on one variable, we will have:\n",
    "\n",
    "$$ \\textbf{y}  =  a_0+ a_1 \\textbf{x}_1 $$\n",
    "\n",
    "With a parameter $a_0$ called constant or cut with the ordinate axis.\n",
    "\n",
    "If we have a **multivariate/multiple linear regression**, we will have:\n",
    "$$ \\textbf{y} = a_1 \\textbf{x}_1 + \\dots + a_m \\textbf{x}_m = X \\textbf{w} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Simple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at some examples in graphic form.\n",
    "\n",
    "We want to predict the weight of a person, using as a independent variable the height. In order to do so, we will create a Simple Linear Regression Model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Height - weight  dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "df = pd.read_csv(\"Data/height-weight-regression.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"Index\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f0f0f0; padding: 25px; border-radius: 5px; margin-top: 25px;\">\n",
    "\n",
    "Plot the data\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## matplotlib and seaborn for visaulizations (seaborn is based on matplotlib)\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"notebook\", font_scale=1, rc={\"lines.linewidth\": 2,'font.family': [u'times']})\n",
    "\n",
    "plt.rc('font', size=12) \n",
    "plt.rc('figure', figsize = (12, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 200\n",
    "\n",
    "x =  df.Height_Inches[:n_samples]\n",
    "y = df.Weight_Pounds[:n_samples]\n",
    "\n",
    "plt.plot(x, y, \"o\", alpha=0.3, color='blue') #being alpha the darkness of the markers\n",
    "# Add axis labels\n",
    "plt.xlabel(\"Height (Inches)\")\n",
    "plt.ylabel(\"Weight (Pounds)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try to find some regression model that fits this datapoints\n",
    "\n",
    "$$ \\textbf{y}  =  a_0+ a_1 \\textbf{x}_1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that there is a certain correlation between them that we could see with this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Setting Model Parameters. Here, a0 and a1 represent the intercept and slope of the line, respectively. \n",
    "a0 = 10\n",
    "a1 = 1.5\n",
    "\n",
    "# Creating the Model Predictions:\n",
    "model=[a0 + a1*x for x in np.arange(64,75)] \n",
    "\n",
    "#Plotting the Data Points and Model:\n",
    "plt.plot(x,y, \"o\", alpha=0.3, color='blue')\n",
    "plt.plot(np.arange(64,75), model,'r')\n",
    "\n",
    "# Drawing Error Lines for Each Point:\n",
    "#zip(a,b) will create a tuple of (a,b) pairs, where the first item in each passed iterator is paired together, and then the second item in each passed iterator are paired together etc.\n",
    "for xi, yi in zip(x,y):\n",
    "    plt.plot([xi]*2, [yi, a0 + a1*xi], \"k:\", linewidth=0.75) # plot errors with black markers (k) connected by a dotted line (\":\")\n",
    "\n",
    "plt.xlabel(\"Height (Inches)\")\n",
    "plt.ylabel(\"Weight (Pounds)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **But which model best fits these data?** \n",
    "\n",
    "How do we find the parameters or coefficients of the following equation?\n",
    "\n",
    "$$ \\textbf{y}  =  a_0+ a_1 \\textbf{x}_1 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y, \"o\", alpha=0.3, color='blue')\n",
    "\n",
    "# Setting Model Parameters for 3 Models\n",
    "model1=[0 + 2*x for x in np.arange(64,75)]\n",
    "model2=[-220 + 5*x for x in np.arange(64,75)]\n",
    "model3=[200 - 1*x for x in np.arange(64,75)]\n",
    "\n",
    "# Let's plot the 3 models\n",
    "plt.plot(np.arange(64,75), model1,'r', label='Model1')\n",
    "plt.plot(np.arange(64,75), model2,'g', label='Model2')\n",
    "plt.plot(np.arange(64,75), model3,'y', label='Model3')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective will always be to $\\textbf{minimize}$ the sum of the square of the distance between the real points ($y_j$) and the value of the function ($ŷ_j$)\n",
    "\n",
    "$$\\textbf{ŷ} = a_0+a_1 \\textbf{x}$$\n",
    "\n",
    "If we have the data $(\\textbf{x},\\textbf{y})$, we want to minimize:\n",
    "\n",
    "$$ ||a_0 + a_1 \\textbf{x} -  \\textbf{y} ||^2_2 = \\sum_{j=1}^n (a_0+a_1 x_{j} -  y_j )^2,$$ \n",
    "\n",
    "This expression is known as **sum of squared errors of prediction (SSE)**.\n",
    "\n",
    "The easiest way to find these two parameters is to use the OLS (*Ordinary Least Squares*) algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Norm and euclidian norm: https://ca.wikipedia.org/wiki/Norma_(matem%C3%A0tiques)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1.1 Ordinary Least Squares (OLS)\n",
    "\n",
    "Let's see an example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SciPy library**\n",
    "\n",
    "SciPy is an open-source Python library used for scientific and technical computing. Built on top of the NumPy library, SciPy provides a wide range of mathematical, scientific, and engineering functionality. It is widely used for tasks that require high-performance numerical computations, including optimization, integration, interpolation, eigenvalue problems, algebraic equations, and statistics.\n",
    "\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Example Image](Figures/lambda.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Choose the number of samples to run this example\n",
    "n_samples = 500\n",
    "\n",
    "x =  df.Height_Inches[:n_samples]\n",
    "y = df.Weight_Pounds[:n_samples]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import fmin\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "\n",
    "# Function to minimize\n",
    "# lambda a, x, y: This is a short way to define a function without explicitly using the def(arguments): keyword. \n",
    "    # This lambda function takes three arguments: a, x, and y.\n",
    "sse = lambda a, x, y: np.sum((y - a[0] - a[1]*x) ** 2) # save sse (function minimize)\n",
    "\n",
    "\n",
    "# Initial guess for the parameters [intercept, slope]\n",
    "initial_guess = [0, 1]\n",
    "\n",
    "result_sse = minimize(sse, initial_guess, args=(x, y))\n",
    "\n",
    "# Get the optimal parameters\n",
    "intercept, slope = result_sse.x\n",
    "\n",
    "\n",
    "print(\"Optimal a0 (incercept): \", intercept)\n",
    "print(\"Optimal a1 (slope): \", slope)\n",
    "\n",
    "\n",
    "print('\\nSSE = ', np.sum((y - intercept - slope*x) ** 2))\n",
    "print('\\nFinal model:\\n',round(intercept,2),'+',round(slope,2),'x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_sse.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data and the linear regression line\n",
    "plt.plot(x, y, 'ro')  #'ro' red circles\n",
    "plt.plot([min(x), max(x)], [intercept + slope * min(x), intercept + slope * max(x)], alpha=0.8, label=\"Regression line SSE\")  # Line plot\n",
    "plt.legend()\n",
    "plt.xlabel(\"Height (Inches)\")\n",
    "plt.ylabel(\"Weight (Pounds)\")\n",
    "plt.title(\"Ordinary Least Squares (OLS) Regression\")\n",
    "\n",
    "for xi, yi in zip(x,y):\n",
    "    plt.plot([xi]*2, [yi, intercept + slope*xi], \"k:\", linewidth=0.75) # show errors\n",
    "plt.xlim(min(x)-1,  max(x)+1); plt.ylim( min(y)-1,  max(y)+1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We could also minimize other values as **sum of the absolute value of the differences**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to minimize: the ABSOLUTE error\n",
    "sabs = lambda a, x, y: np.sum(np.abs(y - a[0] - a[1]*x)) ## function to minize has changed\n",
    "\n",
    "# Initial guess for the parameters [intercept, slope]\n",
    "initial_guess = [0, 1]\n",
    "\n",
    "result_sabs = minimize(sabs, initial_guess, args=(x, y))\n",
    "\n",
    "# Get the optimal parameters\n",
    "intercept, slope = result_sabs.x\n",
    "\n",
    "\n",
    "print(\"Optimal a0 (incercept): \", intercept)\n",
    "print(\"Optimal a1 (slope): \", slope)\n",
    "\n",
    "\n",
    "print('\\nSSE = ', np.sum((y - intercept - slope*x) ** 2))\n",
    "print('\\nAbsolute Errors = ', np.sum(np.abs(y - intercept - slope*x)))\n",
    "print('\\nFinal model:\\n',round(intercept,2),'+',round(slope,2),'x')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_sse.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the data and the linear regression line\n",
    "plt.plot(x, y, 'ro')  #'ro' red circles\n",
    "plt.plot([min(x), max(x)], [intercept + slope * min(x), intercept + slope * max(x)], alpha=0.8, label=\"Regression line SABS\")  # Line plot\n",
    "plt.plot([min(x), max(x)], [result_sse.x[0] + result_sse.x[1] * min(x), result_sse.x[0] + result_sse.x[1] * max(x)], alpha=0.8, label=\"Regression line SSE\", linewidth=1)  \n",
    "plt.legend()\n",
    "plt.xlabel(\"Height (Inches)\")\n",
    "plt.ylabel(\"Weight (Pounds)\")\n",
    "plt.title(\"Ordinary Least Squares (OLS) Regression\")\n",
    "\n",
    "result_sse.x[0]\n",
    "\n",
    "for xi, yi in zip(x,y):\n",
    "    plt.plot([xi]*2, [yi, intercept + slope*xi], \"k:\", linewidth=0.75) # show errors\n",
    "    \n",
    "\n",
    "plt.xlim(min(x)-1,  max(x)+1); plt.ylim( min(y)-1,  max(y)+1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we penalize less for distant values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantages OLS\n",
    "\n",
    "+ Computationally easy to calculate for small datasets. For larger datasets the computation of an inverse causes an increase in computation time.\n",
    "+ Easy to interpret\n",
    "\n",
    "And the model obtained is:\n",
    "\n",
    "$$\\widehat{\\textbf{y}} = \\widehat{a}_0+\\widehat{a}_1 \\textbf{x}$$\n",
    "\n",
    "Hats indicate that these are estimated values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Linear Regression with Sklearn\n",
    "\n",
    "Luckily, we don't have to develop these algorithms ourselves from scratch. That's what machine learning libraries are already made for!\n",
    "\n",
    "For example, let's see how easy it is to create a linear regression model in Sklearn by loading a sample dataset from the library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = x.values.reshape(-1, 1)  # transform in 2D\n",
    "y = y\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 850)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "regr = LinearRegression() #uses ordinary least squares regression\n",
    "\n",
    "# train\n",
    "regr.fit(X_train, y_train) # Fit linear model\n",
    "\n",
    "# Check model\n",
    "print('a1 : \\n', regr.coef_) #\\n for new line\n",
    "print('a0 : \\n', regr.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the data and the linear regression line\n",
    "plt.plot(x, y, 'ro', alpha = 0.5)  #'ro' red circles\n",
    "plt.plot([min(x), max(x)], [intercept + slope * min(x), intercept + slope * max(x)], alpha=0.8, label=\"Regression line SABS\")  # Line plot\n",
    "plt.plot([min(x), max(x)], [result_sse.x[0] + result_sse.x[1] * min(x), result_sse.x[0] + result_sse.x[1] * max(x)], alpha=0.8, label=\"Regression line SSE\", linewidth=1)\n",
    "plt.plot([min(x), max(x)], [regr.intercept_ + regr.coef_ * min(x), regr.intercept_ + regr.coef_ * max(x)], alpha=0.8, label=\"Regression ScikitLearn\", linewidth=1)  \n",
    "plt.legend()\n",
    "plt.xlabel(\"Height (Inches)\")\n",
    "plt.ylabel(\"Weight (Pounds)\")\n",
    "plt.title(\"Ordinary Least Squares (OLS) Regression\")\n",
    "\n",
    "\n",
    "for xi, yi in zip(x,y):\n",
    "    plt.plot([xi]*2, [yi, intercept + slope*xi], \"k:\", linewidth=0.75) # show errors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is obtained with Scikit Learn, we can also make predictions directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regr.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation metrics\n",
    "\n",
    "The model obtained can be evaluated by calculating the **mean squared error** ($MSE$) and the **coefficient of determination** $R^2$.\n",
    "\n",
    "The MSE is calculated as:\n",
    "\n",
    "$$MSE=\\frac{1}{n} \\sum_{i=1}^n (\\widehat{y}^i-y^i)^2,$$ \n",
    "\n",
    "The coefficient $R^2$ is defined as follows:\n",
    "\n",
    "$$(1 - \\textbf{u}/\\textbf{v})$$ \n",
    "\n",
    "where $\\textbf{u}$ is the sum of the squares of the errors: $\\textbf{u}=\\sum (\\textbf{y} - \\widehat{\\textbf{y}} )^2$ where ${\\textbf{y}}$ are the observed values and $\\widehat{\\textbf{y}}$ are the predicted values.\n",
    "\n",
    "and $\\textbf{v}$ is: $$\\textbf{v}=\\sum (\\textbf{y} - \\bar{\\textbf{y}})^2,$$ where $\\bar{\\textbf{y}}$ is the mean of the observed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Sklearn, we could do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "print('Mean squared error (MSE): %.2f'\n",
    "      % mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print('Root Mean squared error (RMSE): %.2f'\n",
    "      % np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print('R2_score: %.2f'\n",
    "      % r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual = y_test-y_pred\n",
    "residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the data and the linear regression line\n",
    "\n",
    "plt.scatter(residual.index, residual, alpha=0.8, label=\"\", linewidth=1)  \n",
    "\n",
    "# Draw a solid line at y=0\n",
    "plt.axhline(0, color='red', linestyle='-', linewidth=1)\n",
    "\n",
    "# Add labels and title (optional)\n",
    "plt.title('Residual Plot')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Residuals')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization\n",
    "\n",
    "We can use Seaborn's ``lmplot()`` function to visualize linear relationships of multidimensional datasets. The input must be in *Pandas* ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example:  Macroeconomic dataset\n",
    "\n",
    "\n",
    "It contains U.S. macroeconomic data from 1947 to 1962, specifically focusing on factors that may influence employment numbers.\n",
    "\n",
    "* Year: The year the data was collected.\n",
    "* Employed: The number of people employed (in thousands).\n",
    "* GNP.deflator: The gross national product implicit price deflator (1954 = 100).\n",
    "* GNP: The gross national product (in millions of dollars).\n",
    "* Unemployed: The number of people unemployed (in thousands).\n",
    "* Armed.Forces: The size of the armed forces (in thousands).\n",
    "* Population: The population of the U.S. (in thousands)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "# Read data\n",
    "df = pd.read_csv('http://vincentarelbundock.github.io/Rdatasets/csv/datasets/longley.csv', index_col=0)\n",
    "df.head()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Macroeconomic data from 1947 to 1962.\n",
    "\n",
    "We want to predict ('Employed') as response $\\textbf{y}$ using ('GNP') as predictor $\\textbf{x}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``lmplot`` is a function from the ``seaborn`` library that stands for \"Linear Model Plot\". It's a versatile function for visualizing data along with a linear regression model, which can be either a simple linear regression or a more complex polynomial regression, depending on how you use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x=\"GNP\", y=\"Employed\", data=df, aspect=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x=\"GNP\", y=\"Population\", data=df, aspect=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x=\"Armed.Forces\", y=\"Unemployed\", data=df, aspect=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are parts that are not very \"linear\".\n",
    "\n",
    "For this we can use polynomial regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Polynomial Regression\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although it is called *linear* regression, we can also fit non-linear functions. The regression will be linear in its parameters not necessarily in its predictors. If nonlinear transformations are added to the linear regression model, the model may become nonlinear\n",
    "\n",
    "$$ \\textbf{y} = a_1 \\phi(\\textbf{x}_1) + \\dots + a_m \\phi(\\textbf{x}_m) $$\n",
    "\n",
    "This technique is known as *Polynomial Regression*, where the higher the degree of the polynomial applied the more complex the model can be (watch out for overfitting!!! and computation time!!!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, a cubic model:\n",
    "\n",
    "$$y_i \\approx a_0 + a_1 x_i + a_2 x_i^2 + a_3 x_i^3$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase the order to estimate a polynomial regression\n",
    "sns.lmplot(x=\"Armed.Forces\", y=\"Unemployed\", data=df, order=2, aspect=2)\n",
    "sns.lmplot(x=\"Armed.Forces\", y=\"Unemployed\", data=df, order=3, aspect=2)\n",
    "sns.lmplot(x=\"Armed.Forces\", y=\"Unemployed\", data=df, order=4, aspect=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

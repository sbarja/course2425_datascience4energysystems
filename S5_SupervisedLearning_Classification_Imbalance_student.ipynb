{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acd3aec8",
   "metadata": {},
   "source": [
    "# Supervised Learning. Classification: Class imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45e951b",
   "metadata": {},
   "source": [
    "### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524ba021",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385df777",
   "metadata": {},
   "source": [
    "## Load the dataset\n",
    "\n",
    "Dataset can be found in [Kaggle](#https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset) \n",
    "\n",
    "The target variable **Diabetes_binary** has 2 classes. \n",
    "* 0 is for no diabetes, \n",
    "* 1 is for prediabetes or diabetes. \n",
    "\n",
    "\n",
    "This dataset has 21 feature variables and is not balanced.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d158dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/S4_diabetes_desequilibrio_clases.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b07bfe3",
   "metadata": {},
   "source": [
    "How many data we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f2a8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ed1799",
   "metadata": {},
   "source": [
    "Let's check more info about the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2e0a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ed72c8",
   "metadata": {},
   "source": [
    "### What represents each column?\n",
    "\n",
    "**(LABEL) Diabetes_binary:** Description: Indicates whether an individual has diabetes (1) or not (0). This is likely the target or outcome variable of the dataset.\n",
    "\n",
    "\n",
    "**HighBP:** Indicates if the person has high blood pressure (1) or not (0).\n",
    "\n",
    "\n",
    "**HighChol:** Indicates if the individual has high cholesterol levels (1) or not (0).\n",
    "\n",
    "**CholCheck:** Whether the individual has had their cholesterol checked (1) in the last 5 years or not (0).\n",
    "\n",
    "**BMI:** The individualâ€™s Body Mass Index (BMI), a measure of body fat based on height and weight.\n",
    "\n",
    "**Smoker:** Indicates whether the individual is a smoker (1) or not (0).\n",
    "\n",
    "\n",
    "**Stroke:** Indicates if the individual has had a stroke (1) or not (0).\n",
    "\n",
    "\n",
    "**HeartDiseaseorAttack:** Whether the individual has had coronary heart disease or myocardial infarction (1) or not (0).\n",
    "\n",
    "**PhysActivity:** Indicates if the individual has engaged in physical activity or exercise in the last 30 days - not including job (1) or not (0).\n",
    "\n",
    "\n",
    "**Fruits:** Indicates whether the individual consumes 1 fruit or more daily (1) or not (0).\n",
    "\n",
    "\n",
    "**Veggies:** Indicates whether the individual consumes 1 vegetable or more daily (1) or not (0).\n",
    "\n",
    "**HeavyAlcoholCons:** Heavy drinkers (adult men having more than 14 drinks per week and adult women having more than 7 drinks per week) (1) or not (0)\n",
    "\n",
    "**AnyHealthcare:** Indicates whether the person has access to any form of healthcare coverage (1) or not (0).\n",
    "\n",
    "**NoDocbcCost:** Whether the person could not visit a doctor in the past year due to cost (1) or not (0).\n",
    "\n",
    "**GenHlth:** Self-reported general health status, where lower values (1) indicate better health (e.g., excellent) and higher values (5) indicate worse health (e.g., poor).\n",
    "\n",
    "**MentHlth:** Number of days in the past 30 days that the individual experienced poor mental health (stress, depression, and problems with emotions).\n",
    "\n",
    "**PhysHlth:** Number of days in the past 30 days that the individual experienced poor physical health (physical illness and injury).\n",
    "\n",
    "**DiffWalk:** Indicates whether the individual has difficulty walking or climbing stairs (1) or not (0).\n",
    "\n",
    "\n",
    "**Sex:** Gender of the individual, 0 = female; 1 = male.\n",
    "\n",
    "**Age:** Represents age group categories. 13-level age category. \n",
    "* 1: 18-24 years old\n",
    "* 2: 25-29 years old\n",
    "* ...\n",
    "* 13: 80 years or older\n",
    "\n",
    "**Education:** Represents the highest level of education attained. \n",
    "* 1: Never attended school\n",
    "* 2: Elementary school\n",
    "* ...\n",
    "* 6: College graduate.\n",
    "\n",
    "**Income:** Represents income categories. Scale 1-8\n",
    "\n",
    "* 1: Less than 10 000 \n",
    "* 2: 10 000 to 14 999\n",
    "* ...\n",
    "* 8: 75 000 or more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac645f4",
   "metadata": {},
   "source": [
    "Show some statistic summary of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfefa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07d1057",
   "metadata": {},
   "source": [
    "### Since this is a binary classification problem, we need to know if there is a class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da7d78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Diabetes_binary'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557266c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "x = data['Diabetes_binary'].value_counts().values\n",
    "labels = data['Diabetes_binary'].value_counts().index\n",
    "\n",
    "# Use the correct syntax for barplot\n",
    "sns.barplot(x=labels, y=x)\n",
    "plt.title('Frequency Table of the Label')\n",
    "plt.xlabel('Diabetes Binary')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Print the total number of labels\n",
    "print('Total number of labels: ', sum(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d92fd4",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<div style=\"background-color:#ccffcc; padding:10px; border-radius:5px;\">\n",
    "\n",
    "### <span style=\"color:blue\">Exercise 1</span>\n",
    "What's the percentage of the minor class?\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63722cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# write your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7b1d25",
   "metadata": {},
   "source": [
    "It is evident that there is a class imbalance problem. Now, we will study the methods to address this problem.\n",
    "\n",
    "# **Methods to handle imbalance datasets**: \n",
    "* Resampling: \n",
    "    * Oversampling\n",
    "    * Undersampling\n",
    "    \n",
    "* Oversampling with synthetic data \n",
    "* Using class weight in model training\n",
    "* Change classification evaluation metric to identify class imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a22b404",
   "metadata": {},
   "source": [
    "## 1. Resampling \n",
    "\n",
    "\n",
    "`sklearn.utils.resample(*arrays, replace=True, n_samples=None, random_state=None, stratify=None)`\n",
    "\n",
    "[Scikit learn documentation](#https://scikit-learn.org/1.5/modules/generated/sklearn.utils.resample.html)\n",
    "\n",
    "\n",
    "A widely adopted technique for dealing with highly imbalanced datasets is called resampling. It involves removing samples from the majority class (undersampling) and/or adding more examples from the minority class (oversampling).\n",
    "\n",
    "<img src=\"Figures/resampling.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcba385",
   "metadata": {},
   "source": [
    "### **1.1 Random Oversampling the Minority Class**\n",
    "\n",
    "Oversampling can be defined as adding more copies of the minority class. Oversampling can be a good option when there isn't a lot of data to work with.\n",
    "\n",
    "We will use the resampling module from Scikit-Learn to randomly replicate samples from the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433bd86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "no_diabetes_majority_class = data[data[\"Diabetes_binary\"] == 0]\n",
    "diabetes_minority_class = data[data[\"Diabetes_binary\"] == 1]\n",
    "\n",
    "# upsample minority\n",
    "diabetes_minority_class_upsampled = resample(diabetes_minority_class,\n",
    "                          replace=True, # This has to be set True when upsampling\n",
    "                          n_samples=len(no_diabetes_majority_class), # match number in majority class\n",
    "                          random_state=27) # reproducible results\n",
    "\n",
    "# combine majority and upsampled minority\n",
    "upsampled_randomoversampling = pd.concat([no_diabetes_majority_class, diabetes_minority_class_upsampled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2f97cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "upsampled_randomoversampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4974180a",
   "metadata": {},
   "source": [
    "###  Plot the Frequency table\n",
    "\n",
    "Now, let's use the Seaborn library palette. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180d038d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "x = upsampled_randomoversampling['Diabetes_binary'].value_counts().values\n",
    "labels = upsampled_randomoversampling['Diabetes_binary'].value_counts().index\n",
    "\n",
    "# Define a color palette\n",
    "palette = sns.color_palette(\"Blues_r\", len(labels))\n",
    "\n",
    "# Create the barplot \n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x=labels, y=x, palette=palette)\n",
    "\n",
    "# Add titles and labels with font customizations\n",
    "plt.title('Frequency Table of the Label: Upsampled (Random Oversampling)', fontsize=16, fontweight='bold', color='navy')\n",
    "plt.xlabel('Diabetes Binary', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "\n",
    "# Add value annotations on top of the bars\n",
    "for i in range(len(x)):\n",
    "    plt.text(i, x[i] + 200, str(x[i]), ha='center', fontsize=10, color='black')\n",
    "\n",
    "# Customize grid and aesthetics\n",
    "sns.despine(top=True, right=True, left=False, bottom=False)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Total number of labels: ', x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71848f7a",
   "metadata": {},
   "source": [
    "### **1.2 Random  Undersampling the Majority Class**\n",
    "\n",
    "Undersampling can be defined as the removal of some observations from the majority class. Undersampling can be a good option when there is a large amount of data available, for example, millions of rows. However, the downside is that we are eliminating information that could be valuable. This could lead to underfitting and poor generalization on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd04268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar to upsampling\n",
    "no_diabetes_majority_class = data[data[\"Diabetes_binary\"] == 0]\n",
    "diabetes_minority_class = data[data[\"Diabetes_binary\"] == 1]\n",
    "\n",
    "\n",
    "no_diabetes_majority_class_downsampled = resample(no_diabetes_majority_class,\n",
    "                                replace = False, # sample without replacement\n",
    "                                n_samples = len(diabetes_minority_class), # match minority n\n",
    "                                random_state = 27) # reproducible results\n",
    "\n",
    "# combine minority and downsampled majority\n",
    "downsampled_randomsampling = pd.concat([no_diabetes_majority_class_downsampled, diabetes_minority_class])\n",
    "\n",
    "# checking counts\n",
    "downsampled_randomsampling.Diabetes_binary.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a602d5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "x = downsampled_randomsampling['Diabetes_binary'].value_counts().values\n",
    "labels = downsampled_randomsampling['Diabetes_binary'].value_counts().index\n",
    "\n",
    "\n",
    "sns.barplot(x=labels, y=x)\n",
    "plt.title('Frequency Table of the Label: downsampled')\n",
    "plt.xlabel('Diabetes Binary')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "print('Total number of labels: ', x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c2e5e8",
   "metadata": {},
   "source": [
    "## 2. Synthetic Data\n",
    "\n",
    "Several more sophisticated resampling techniques have been proposed in the literature.\n",
    "\n",
    "For example, in oversampling, instead of creating exact copies of records from the minority class, we can introduce small variations in those copies, creating more diverse synthetic samples.\n",
    "\n",
    "We are going to apply this resampling technique (creation of synthetic data) using the **[imbalanced-learn](https://imbalanced-learn.org/stable/)** Python library. It is compatible with scikit-learn and is part of the scikit-learn-contrib projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40535340",
   "metadata": {},
   "source": [
    "### Over-sampling: SMOTE\n",
    "\n",
    "**Synthetic Minority Oversampling Technique (SMOTE)** \n",
    "\n",
    "SMOTE is a more sophisticated technique than random oversampling. It generates synthetic samples for the minority class by interpolating between existing samples.\n",
    "\n",
    "<img src=\"Figures/SMOTE.png\" alt=\"Drawing\" style=\"width: 400px\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6debfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you have not installed imblearn, run the line below\n",
    "\n",
    "# !pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad578c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Let's separate the features from the target column.\n",
    "X = data.drop('Diabetes_binary', axis=1)  # Features (all columns except target)\n",
    "y = data['Diabetes_binary']  # Target column (Diabetes_binary)\n",
    "\n",
    "# Apply SMOTE to upsample the minority class\n",
    "smote = SMOTE(random_state=27)  # Initializes the SMOTE oversampler with a random seed for reproducibility.\n",
    "X_smote, y_smote = smote.fit_resample(X, y)  #  Applies SMOTE to balance the minority class by generating synthetic samples.\n",
    "\n",
    "# Combine X_smote and y_smote into a new DataFrame\n",
    "upsampled_smote = pd.concat([pd.DataFrame(X_smote, columns=X.columns), pd.DataFrame(y_smote, columns=['Diabetes_binary'])], axis=1)\n",
    "\n",
    "# Display the balanced dataset\n",
    "print(upsampled_smote['Diabetes_binary'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53192199",
   "metadata": {},
   "outputs": [],
   "source": [
    "upsampled_smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081631b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "upsampled_smote.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404f288a",
   "metadata": {},
   "source": [
    "## 3. Using algorithms with Class Weights\n",
    "\n",
    "Many Scikit-learn classifiers support the class_weight parameter to help deal with class imbalance:\n",
    "\n",
    "* Logistic Regression: ``LogisticRegression(class_weight='balanced')``\n",
    "* Decision Trees: ``DecisionTreeClassifier(class_weight='balanced')``\n",
    "* Random Forest: ``RandomForestClassifier(class_weight='balanced')``\n",
    "* Support Vector Machines: `SVC(class_weight='balanced')`\n",
    "* Gradient Boosting: `GradientBoostingClassifier(class_weight='balanced')`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39731390",
   "metadata": {},
   "source": [
    "### Train the model using the ML classification models \n",
    "\n",
    "### But first, let's build a simple classification model using the original dataset, without random sampling or ``class_weight='balanced'``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8969f230",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "\n",
    "# load again the dataset\n",
    "data = pd.read_csv('data/S4_diabetes_desequilibrio_clases.csv')\n",
    "\n",
    "X = data.iloc[:,1:]\n",
    "y = data[\"Diabetes_binary\"]\n",
    "\n",
    "# Splitting the data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e4a851",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea3eae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1beec93",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbaadb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f1020c",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74572c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "y_pred_logreg = log_reg.predict(X_test)\n",
    "acc_lg =  accuracy_score(y_test, y_pred_logreg)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Logistic Regression Accuracy: \", acc_lg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3509089",
   "metadata": {},
   "source": [
    "###  Support Vector Classifier (SVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65ce9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# SVC\n",
    "svc = SVC(kernel = 'linear', random_state=42, shrinking =True, verbose=True, tol=0.1, max_iter=100)\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_svc = svc.predict(X_test)\n",
    "acc_svc = accuracy_score(y_test, y_pred_svc)\n",
    "# Evaluation\n",
    "print(\"SVC Accuracy: \", acc_svc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4c1b38",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fb5c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Decision Tree\n",
    "decision_tree = DecisionTreeClassifier(random_state=42)\n",
    "decision_tree.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_tree = decision_tree.predict(X_test)\n",
    "acc_dt = accuracy_score(y_test, y_pred_tree)\n",
    "# Evaluation\n",
    "print(\"Decision Tree Accuracy: \", acc_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fd22c1",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4cc445",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Random Forest\n",
    "random_forest = RandomForestClassifier(random_state=42)\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_forest = random_forest.predict(X_test)\n",
    "acc_rf = accuracy_score(y_test, y_pred_forest)\n",
    "# Evaluation\n",
    "print(\"Random Forest Accuracy: \", acc_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7670258e",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de7d102",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Convert X_train and X_test to NumPy arrays\n",
    "X_train_np = X_train.to_numpy()\n",
    "X_test_np = X_test.to_numpy()\n",
    "\n",
    "# KNN\n",
    "knn = KNeighborsClassifier(n_neighbors=7)\n",
    "knn.fit(X_train_np, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_knn = knn.predict(X_test_np)\n",
    "\n",
    "acc_knn = accuracy_score(y_test, y_pred_knn)\n",
    "\n",
    "# Evaluation\n",
    "print(\"KNN Accuracy: \", acc_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f814c2a",
   "metadata": {},
   "source": [
    "### First approach results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11259ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Create a DataFrame with the results\n",
    "accuracy_data = {\n",
    "    \"Model\": [\"KNN\", \"Random Forest\", \"Decision Tree\", \"SVC\", \"Logistic Regression\"],\n",
    "    \"Accuracy\": [acc_knn, acc_rf, acc_dt, acc_svc, acc_lg]\n",
    "}\n",
    "\n",
    "accuracy_df = pd.DataFrame(accuracy_data)  # Seaborn library expects a dataframe as input\n",
    "\n",
    "# Plotting using Seaborn\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=\"Model\", y=\"Accuracy\", data=accuracy_df, palette=\"Blues_d\")\n",
    "plt.title('Accuracy Comparison of Different Classification Models')\n",
    "plt.xlabel('Classification Models')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1)  # Ensure the y-axis goes from 0 to 1 for better visualization\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bca8656",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<div style=\"background-color:#ffffaa; padding:10px; border-radius:5px;\">\n",
    "\n",
    "### <span style=\"color:blue\">(Optional to practice) </span>\n",
    "Add more ensemble models and compare\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fdc460",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3392ecf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad03bffe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd0e0b95",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"background-color:#ffffaa; padding:10px; border-radius:5px;\">\n",
    "\n",
    "### <span style=\"color:blue\">(Optional to practice) </span>\n",
    "Run all these examples again, but with the class_weight='balanced'. Are the results better?\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b6cf16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de72b1d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce0149b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31264ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5d0452b",
   "metadata": {},
   "source": [
    "## 4. With imbalance classes, we need to check more Evaluation Metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f71d70",
   "metadata": {},
   "source": [
    "As we have seen above, accuracy is not the best metric for evaluating unbalanced data sets, as it can be misleading. Metrics that can provide better insight include:\n",
    "\n",
    "**Confusion matrix**: a table showing correct predictions and types of incorrect predictions.\n",
    "\n",
    "* **Accuracy**: the number of true positives divided by all positive predictions. Precision is also called positive predictive value. It is a measure of the accuracy of a classifier. A low precision indicates a high number of false positives.\n",
    "\n",
    "* **Recall**: the number of true positives divided by the number of positive values in the test data, also called sensitivity or true positive rate. It is a measure of the completeness of a classifier. A low recall indicates a high number of false negatives.\n",
    "\n",
    "* **F1 score**: the weighted average of precision and recall.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecf6c91",
   "metadata": {},
   "source": [
    "\n",
    "**Precision for Class 1 (Diabetes):**\n",
    "* Precision tells us how many of the instances predicted as diabetes (class 1) are actually diabetic.\n",
    "* For isntance, if precision for class 1 is 0.42, this means that when the model predicts someone has diabetes, it is correct 42% of the time. The remaining 58% of the predictions are false positives (people without diabetes incorrectly predicted as having diabetes).\n",
    "\n",
    "**Recall for Class 1 (Diabetes):**\n",
    "* Recall (also known as sensitivity or true positive rate) measures how many of the actual diabetic cases were correctly identified by the model.\n",
    "* Recall for class 1 is 0.17, meaning that the model correctly identifies only 17% of the people who actually have diabetes.\n",
    "\n",
    "**F1-Score for Class 1 (Diabetes):**\n",
    "* The F1-score is the harmonic mean of precision and recall, combining both into a single metric. It balances the two metrics, making it useful when the data is imbalanced, as is the case here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b34e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "# Function to plot confusion matrix\n",
    "def plot_confusion_matrix(y_test, y_pred, model_name):\n",
    "    cm = confusion_matrix(y_pred, y_test)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=sns.cubehelix_palette(as_cmap=True), cbar=False)\n",
    "    plt.title(f\"Confusion Matrix - {model_name}\")\n",
    "    plt.ylabel('Predicted Label')\n",
    "    plt.xlabel('Actual Label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26760aa4",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f0fef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "data = pd.read_csv('data/S4_diabetes_desequilibrio_clases.csv')\n",
    "\n",
    "X = data.iloc[:,1:]\n",
    "y = data[\"Diabetes_binary\"]\n",
    "\n",
    "# Splitting the data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "y_pred_logreg = log_reg.predict(X_test)\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "print(\"Logistic Regression Accuracy: \", accuracy_score(y_test, y_pred_logreg))\n",
    "print(classification_report(y_test, y_pred_logreg))\n",
    "print(confusion_matrix(y_test, y_pred_logreg))\n",
    "plot_confusion_matrix(y_test, y_pred_logreg, \"Logistic Regression\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bf2747",
   "metadata": {},
   "source": [
    "###  Support Vector Classifier (SVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375ea8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# SVC\n",
    "svc = SVC(kernel = 'linear', random_state=42, shrinking =True, verbose=True, tol=0.1, max_iter=100)\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_svc = svc.predict(X_test)\n",
    "acc_svc = accuracy_score(y_test, y_pred_svc)\n",
    "\n",
    "# Evaluation\n",
    "print(\"SVC Accuracy: \", acc_svc)\n",
    "print(classification_report(y_test, y_pred_svc))\n",
    "print(confusion_matrix(y_test, y_pred_svc))\n",
    "plot_confusion_matrix(y_test, y_pred_svc, \"SVC\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016b2f7c",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ea1539",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "data = pd.read_csv('data/S4_diabetes_desequilibrio_clases.csv')\n",
    "\n",
    "\n",
    "X = data.iloc[:,1:]\n",
    "y = data[\"Diabetes_binary\"]\n",
    "\n",
    "# Splitting the data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "# Decision Tree\n",
    "decision_tree = DecisionTreeClassifier(random_state=42)\n",
    "decision_tree = decision_tree.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_tree = decision_tree.predict(X_test)\n",
    "acc_dt = accuracy_score(y_test, y_pred_tree)\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "print(\"Decision Tree Accuracy: \", acc_dt)\n",
    "print(classification_report(y_test, y_pred_tree))\n",
    "print(confusion_matrix(y_test, y_pred_tree))\n",
    "plot_confusion_matrix(y_test, y_pred_tree, \"Decision Tree\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0656e98",
   "metadata": {},
   "source": [
    "With decision trees and random forest, we can see the real tree\n",
    "\n",
    "Visualize decision tree in different ways [source](#https://mljar.com/blog/visualize-decision-tree/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf09342",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_text\n",
    "\n",
    "# Textual representation of the decision tree\n",
    "text_representation = export_text(decision_tree, feature_names=X.columns.tolist())\n",
    "print(text_representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05701d7e",
   "metadata": {},
   "source": [
    "### Random Forest Classifier\n",
    "\n",
    "Not too complex random forest to plot the tree in a reasonable time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6bbaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, matthews_corrcoef\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "data = pd.read_csv('data/S4_diabetes_desequilibrio_clases.csv')\n",
    "\n",
    "X = data.iloc[:,1:]\n",
    "y = data[\"Diabetes_binary\"]\n",
    "\n",
    "# Splitting the data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# Random Forest\n",
    "random_forest = RandomForestClassifier(n_estimators=10, max_depth=5, random_state=42)\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_forest = random_forest.predict(X_test)\n",
    "acc_rf = accuracy_score(y_test, y_pred_forest)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Random Forest Accuracy: \", acc_rf)\n",
    "print(classification_report(y_test, y_pred_forest))\n",
    "print(confusion_matrix(y_test, y_pred_forest))\n",
    "plot_confusion_matrix(y_test, y_pred_forest, \"Random Forest\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f31f452",
   "metadata": {},
   "source": [
    "### Plot the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a366e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import tree\n",
    "\n",
    "# Extract one tree from the Random Forest\n",
    "estimator = random_forest.estimators_[0]\n",
    "\n",
    "# Convert feature names to a list\n",
    "feature_names = X.columns.tolist()\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (50,10), dpi=800)\n",
    "tree.plot_tree(estimator,\n",
    "               feature_names = feature_names, \n",
    "               class_names=[\"No Diabetes\", \"Diabetes\"],\n",
    "               filled = True);\n",
    "fig.savefig('rf_individualtree.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b23d8a",
   "metadata": {},
   "source": [
    "## Which are the most relevant features for this model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ed5b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate feature importances\n",
    "importances = random_forest.feature_importances_\n",
    "feature_names = X.columns.tolist()\n",
    "\n",
    "\n",
    "# Create a DataFrame for feature importances\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f4ce77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by importance\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plotting the feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance_df)\n",
    "plt.title('Feature Importances from Random Forest Model')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()\n",
    "\n",
    "# Print the most relevant features\n",
    "print(\"Most Relevant Features:\")\n",
    "print(feature_importance_df.head(10))  # Show top 10 features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e6ae15",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a37d64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Convert X_train and X_test to NumPy arrays\n",
    "X_train_np = X_train.to_numpy()\n",
    "X_test_np = X_test.to_numpy()\n",
    "\n",
    "# KNN\n",
    "knn = KNeighborsClassifier(n_neighbors=7)\n",
    "knn.fit(X_train_np, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_knn = knn.predict(X_test_np)\n",
    "\n",
    "acc_knn = accuracy_score(y_test, y_pred_knn)\n",
    "\n",
    "# Evaluation\n",
    "print(\"KNN Accuracy: \", acc_knn)\n",
    "print(classification_report(y_test, y_pred_knn))\n",
    "print(confusion_matrix(y_test, y_pred_knn))\n",
    "plot_confusion_matrix(y_test, y_pred_knn, \"KNN\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b48062",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<div style=\"background-color:#ccffcc; padding:10px; border-radius:5px;\">\n",
    "\n",
    "### <span style=\"color:blue\">Question</span>\n",
    "Which model performs best?\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a80647",
   "metadata": {},
   "source": [
    "## Matthews Correlation Coefficient (MCC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff578a7",
   "metadata": {},
   "source": [
    "The **Matthews Correlation Coefficient (MCC)** is a metric used to evaluate the quality of binary classifications, particularly useful in situations with imbalanced classes.\n",
    "\n",
    "**Interpretation of MCC:**\n",
    "* Range: The MCC value ranges from -1 to 1.\n",
    "    * 1: Perfect classification.\n",
    "    * 0: No better than random guessing.\n",
    "    * -1: Completely wrong classification.\n",
    "    \n",
    "**Advantages of MCC**\n",
    "* Balanced Assessment: It considers all elements of the confusion matrix, providing a more balanced evaluation than precision or recall alone.\n",
    "* Useful for Imbalanced Data: It effectively evaluates models in datasets where one class significantly outweighs the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af457bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "mcc_log = matthews_corrcoef(y_test, y_pred_logreg)\n",
    "mcc_svc = matthews_corrcoef(y_test, y_pred_svc)\n",
    "mcc_tree = matthews_corrcoef(y_test, y_pred_tree)\n",
    "mcc_rf = matthews_corrcoef(y_test, y_pred_forest)\n",
    "mcc_knn = matthews_corrcoef(y_test, y_pred_knn)\n",
    "\n",
    "# Create a DataFrame with the results\n",
    "mcc_data = {\n",
    "    \"Model\": [\"KNN\", \"Random Forest\", \"Decision Tree\", \"SVC\", \"Logistic Regression\"],\n",
    "    \"MCC\": [mcc_knn, mcc_rf, mcc_tree, mcc_svc, mcc_log]\n",
    "}\n",
    "\n",
    "mcc_df = pd.DataFrame(mcc_data)  # Seaborn library expects a dataframe as input\n",
    "\n",
    "# Plotting using Seaborn\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=\"Model\", y=\"MCC\", data=mcc_df, palette=\"Blues_d\")\n",
    "plt.title('MCC Comparison of Different Classification Models')\n",
    "plt.xlabel('Classification Models')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1)  # Ensure the y-axis goes from 0 to 1 for better visualization\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f733fdf",
   "metadata": {},
   "source": [
    "## Random Forest performs best. Let's see if we can improve the model over or undersampling the original dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d17f80",
   "metadata": {},
   "source": [
    "### Random oversampling the minority class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc98b3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, matthews_corrcoef\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Assuming you have your features and target set up\n",
    "X = data.iloc[:, 1:]  # Features\n",
    "y = data[\"Diabetes_binary\"]  # Target\n",
    "\n",
    "# Split the data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db74d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4ed33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the majority and minority classes\n",
    "X_no_diabetes_majority_class_train = X_train[y_train == 0]\n",
    "X_diabetes_minority_class_train = X_train[y_train == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b0e0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_no_diabetes_majority_class_train.shape)\n",
    "print(X_diabetes_minority_class_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca88eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsample minority class\n",
    "X_diabetes_minority_class_upsampled_train = resample(X_diabetes_minority_class_train,\n",
    "                          replace=True,  # sample with replacement\n",
    "                          n_samples=len(X_no_diabetes_majority_class_train),  # match number in majority class\n",
    "                          random_state=42)  # reproducible results\n",
    "\n",
    "X_diabetes_minority_class_upsampled_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdfa8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_no_diabetes_majority_class_train.shape)\n",
    "print(X_diabetes_minority_class_upsampled_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2792f703",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Combine majority class with upsampled minority class\n",
    "X_train_resampled = pd.concat([X_no_diabetes_majority_class_train, X_diabetes_minority_class_upsampled_train])\n",
    "y_train_resampled = pd.concat([y_train[y_train == 0], pd.Series([1] * len(X_diabetes_minority_class_upsampled_train))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45124812",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train the Random Forest Classifier\n",
    "random_forest = RandomForestClassifier(random_state=42)\n",
    "random_forest.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Predictions\n",
    "y_pred_forest = random_forest.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "acc_rf = accuracy_score(y_test, y_pred_forest)\n",
    "print(\"Random Forest Accuracy: \", acc_rf)\n",
    "print(classification_report(y_test, y_pred_forest))\n",
    "print(confusion_matrix(y_test, y_pred_forest))\n",
    "\n",
    "plot_confusion_matrix(y_test, y_pred_forest, \"Random Forest with Oversampling\")\n",
    "\n",
    "# Calculate MCC\n",
    "mcc_rf = matthews_corrcoef(y_test, y_pred_forest)\n",
    "print(\"Matthews Correlation Coefficient (MCC): \", mcc_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10416abf",
   "metadata": {},
   "source": [
    "### Another way of Oversampling: using Imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe49af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, matthews_corrcoef\n",
    "from imblearn.over_sampling import RandomOverSampler  # Importing RandomOverSampler\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# From the original dataset\n",
    "X = data.iloc[:, 1:]  # Features\n",
    "y = data[\"Diabetes_binary\"]  # Target\n",
    "\n",
    "# Step 1: Split the data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)\n",
    "\n",
    "\n",
    "# Step 2: Apply Random Over Sampling form imblearn library\n",
    "oversampler = RandomOverSampler(random_state=42)  # Use RandomOverSampler\n",
    "X_train_resampled, y_train_resampled = oversampler.fit_resample(X_train, y_train)\n",
    "\n",
    "# Step 3: Train the Random Forest Classifier\n",
    "random_forest = RandomForestClassifier(random_state=42)\n",
    "random_forest.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Step 4: Predictions\n",
    "y_pred_forest = random_forest.predict(X_test)\n",
    "\n",
    "# Step 5: Evaluation\n",
    "acc_rf = accuracy_score(y_test, y_pred_forest)\n",
    "print(\"Random Forest Accuracy: \", acc_rf)\n",
    "print(classification_report(y_test, y_pred_forest))\n",
    "print(confusion_matrix(y_test, y_pred_forest))\n",
    "\n",
    "# Step 6: Plot confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Function to plot confusion matrix\n",
    "def plot_confusion_matrix(y_test, y_pred, model_name):\n",
    "    cm = confusion_matrix(y_pred, y_test)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=sns.cubehelix_palette(as_cmap=True), cbar=False)\n",
    "    plt.title(f\"Confusion Matrix - {model_name}\")\n",
    "    plt.ylabel('Predicted Label')\n",
    "    plt.xlabel('Actual Label')\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion_matrix(y_test, y_pred_forest, \"Random Forest with Oversampling\")\n",
    "\n",
    "# Step 7: Calculate and print MCC\n",
    "mcc_rf = matthews_corrcoef(y_test, y_pred_forest)\n",
    "print(\"Matthews Correlation Coefficient (MCC): \", mcc_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb57e541",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "597157bc",
   "metadata": {},
   "source": [
    "###  UNDERSAMPLING: Let's see if undersampling the lowest class, the model improves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be15a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Assuming you have your features and target set up\n",
    "X = data.iloc[:, 1:]  # Features\n",
    "y = data[\"Diabetes_binary\"]  # Target\n",
    "\n",
    "# Split the data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=48, shuffle=True)\n",
    "\n",
    "# Apply Random Under Sampling\n",
    "undersampler = RandomUnderSampler(random_state=42)\n",
    "X_train_resampled, y_train_resampled = undersampler.fit_resample(X_train, y_train)\n",
    "\n",
    "# Train the Random Forest Classifier\n",
    "random_forest = RandomForestClassifier(random_state=42)\n",
    "random_forest.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Predictions\n",
    "y_pred_forest = random_forest.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "acc_rf = accuracy_score(y_test, y_pred_forest)\n",
    "print(\"Random Forest Accuracy: \", acc_rf)\n",
    "print(classification_report(y_test, y_pred_forest))\n",
    "print(confusion_matrix(y_test, y_pred_forest))\n",
    "\n",
    "plot_confusion_matrix(y_test, y_pred_forest, \"Random Forest with Undersampling\")\n",
    "\n",
    "# Step 7: Calculate and print MCC\n",
    "mcc_rf = matthews_corrcoef(y_test, y_pred_forest)\n",
    "print(\"Matthews Correlation Coefficient (MCC): \", mcc_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6411b3e",
   "metadata": {},
   "source": [
    "### Use the weight class balance\n",
    "\n",
    "`RandomForestClassifier(class_weight='balanced', random_state=42)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d77a60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Assuming you have your features and target set up\n",
    "X = data.iloc[:, 1:]  # Features\n",
    "y = data[\"Diabetes_binary\"]  # Target\n",
    "\n",
    "# Split the data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1990, shuffle=True)\n",
    "\n",
    "\n",
    "# Train the Random Forest Classifier\n",
    "random_forest_balanced = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
    "random_forest = RandomForestClassifier(random_state=42)\n",
    "\n",
    "random_forest.fit(X_train, y_train)\n",
    "random_forest_balanced.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_forest = random_forest.predict(X_test)\n",
    "y_pred_forest_balanced = random_forest_balanced.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "plot_confusion_matrix(y_test, y_pred_forest, \"Random Forest\")\n",
    "plot_confusion_matrix(y_test, y_pred_forest_balanced, \"Random Forest Balanced\")\n",
    "\n",
    "# Step 7: Calculate and print MCC\n",
    "mcc_rf = matthews_corrcoef(y_test, y_pred_forest)\n",
    "mcc_rf_balanced = matthews_corrcoef(y_test, y_pred_forest_balanced)\n",
    "print(\"Matthews Correlation Coefficient (MCC): \", mcc_rf)\n",
    "print(\"Matthews Correlation Coefficient (MCC) Balanced: \", mcc_rf_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930ce25c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61add180",
   "metadata": {},
   "source": [
    "## Let's try moving the threshold for logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3010fc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('data/S4_diabetes_desequilibrio_clases.csv')\n",
    "\n",
    "X = data.iloc[:, 1:]\n",
    "y = data[\"Diabetes_binary\"]\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize and fit the logistic regression model\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Get predicted probabilities\n",
    "y_proba = log_reg.predict_proba(X_test)[:, 1]  # Probability for the positive class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc58fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37427f20",
   "metadata": {},
   "source": [
    "Here, a custom threshold of 0.2 is set. This means that any sample with a predicted probability of at least 0.2 will be classified as class 1, and samples with predicted probabilities below 0.2 will be classified as class 0.\n",
    "\n",
    "`.astype(int)`:\n",
    "\n",
    "The ``.astype(int)`` method converts the boolean array (``True/False``) into an integer array (``1/0``). In Python, ``True`` is equivalent to ``1`` and ``False``is equivalent to ``0``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd1233a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set a custom threshold\n",
    "threshold = 0.2  # Example threshold\n",
    "y_pred_custom_threshold = (y_proba >= threshold).astype(int)\n",
    "\n",
    "# Evaluation with custom threshold\n",
    "print(\"Custom Threshold Logistic Regression Accuracy: \", accuracy_score(y_test, y_pred_custom_threshold))\n",
    "print(classification_report(y_test, y_pred_custom_threshold))\n",
    "print(confusion_matrix(y_test, y_pred_custom_threshold))\n",
    "\n",
    "plot_confusion_matrix(y_test, y_pred_custom_threshold, \"Logistic Regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813c9d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_custom_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e32cbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figures/top_ML.png\" alt=\"Drawing\" style=\"width: 1000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXAMPLE\n",
    "\n",
    "Develop a Supervised Machine Learning model to classify the users of a Electricity Retail Company, according to their hourly electricity consumption profile during a day. This classification will allow the company's marketing staff to send personalized and appropriate offers to these two types of customer profiles: users with a **high consumption profile** and users with a **non-high consumption profile**.\n",
    "\n",
    "The columns are (0) CUPs, (1) cluster and (2-26) hourly consumption (from h-0 to h-23)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #import pandas\n",
    "import matplotlib.pyplot as plt # import matplotlib to make graphs\n",
    "import seaborn as sns # import seaborn to make graphics\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>Load the dataset </b>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b> Look for cluster uniques classes </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset shape\n",
    "consumption.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumption.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b> Are there missing value? </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's see how many cases we have in each of the clusters. Do we have a balanced dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster==0\n",
    "print(\"Number cluster 0:\", consumption[consumption['cluster'] == 0]['cluster'].count())\n",
    "# cluster=1\n",
    "print(\"Number cluster 1:\", consumption[consumption['cluster'] == 1]['cluster'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the value counts\n",
    "x = consumption['cluster'].value_counts()\n",
    "\n",
    "# Create the barplot\n",
    "sns.barplot(x=x.index, y=x.values)\n",
    "\n",
    "# Set the title\n",
    "plt.title('Value counts target')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b> Create two dataframes (one for each cluster) to analyze them separately </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clients_0 = ## your code here\n",
    "clients_1 = ## your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average hourly consumption comparison\n",
    "print(\"Average hourly power cluster 0: \", clients_0.drop(['CUPs','cluster'], axis=1).mean(axis=1).mean(), 'kW')\n",
    "print(\"Average hourly power cluster 1: \", clients_1.drop(['CUPs','cluster'], axis=1).mean(axis=1).mean(), 'kW')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove the 'cluster' column in order to plot the different load curves.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0 = clients_0.drop(['cluster'], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make the \"column CUPs\" as index (it makes sense, since each row has a different value and identifies the SM).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0.set_index(['CUPs'], inplace=True)\n",
    "\n",
    "# Transpose the matrix, for ease of plotting\n",
    "df_0 = df_0.T\n",
    "\n",
    "# We change the name of the index to \"hour\".\n",
    "df_0.index.name = 'hour'\n",
    "df_0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = clients_1.drop(['cluster'], axis=1)\n",
    "df_1.set_index(['CUPs'], inplace=True)\n",
    "df_1 = df_1.T\n",
    "df_1.index.name = 'hour'\n",
    "df_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We obtain a list with the columns of the two dfs to have the CUPs of cluster 0 and cluster 1.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cups_0 = df_0.columns\n",
    "cups_1 = df_1.columns\n",
    "\n",
    "print(cups_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(20,8))\n",
    "\n",
    "# Create a loop where cups takes each of the strings in the cups_0 list.\n",
    "for cups in cups_0:\n",
    "    # 'lightcoral' indicates the color (https://matplotlib.org/2.1.1/gallery/color/named_colors.html)\n",
    "    # linewidth sets the line width and alpha the transparency\n",
    "    plt.plot(df_0[cups], 'lightcoral', linewidth=1, alpha=0.4)\n",
    "for cups in cups_1:\n",
    "    plt.plot(df_1[cups], 'green', linewidth=1, alpha=0.4)\n",
    "\n",
    "    # X axis displays the hours\n",
    "plt.xticks(df_0.index)\n",
    "plt.xlabel('Hours', fontsize=16)\n",
    "plt.ylabel('Consumers consumption [kWh]', fontsize=16)\n",
    "\n",
    "plt.margins(x=0, y=0)\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b> Add average consumption to distinguish more clearly the differences between the clusters. </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0['mean'] = ## your code here\n",
    "df_1['mean'] = ## your code here\n",
    "df_0.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We create the same graphs as before, adding the average curves of the two clusters with more opacity (alpha)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(20,8))\n",
    "for cups in cups_0:\n",
    "    plt.plot(df_0[cups], 'lightcoral', linewidth=1, alpha=0.2)\n",
    "\n",
    "for cups in cups_1:\n",
    "    plt.plot(df_1[cups], 'green', linewidth=1, alpha=0.2)\n",
    "\n",
    "plt.plot(df_0['mean'], 'tomato', linestyle='dashed', linewidth=4, alpha=1)    \n",
    "plt.plot(df_1['mean'], 'green', linestyle='dashed', linewidth=4, alpha=1)\n",
    "\n",
    "plt.xticks(df_0.index)\n",
    "plt.margins(x=0, y=0)\n",
    "plt.xlabel('Hours', fontsize=16)\n",
    "plt.ylabel('Consumers consumption [kWh]', fontsize=16)\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Correlation between features and target**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(18, 10))\n",
    "\n",
    "# Create the correlation matrix after eliminating the CUPs column since it does not provide information in this case.\n",
    "corr = consumption.drop(['CUPs'],axis=1).corr()\n",
    "\n",
    "# Create a heat map to visually detect the correlation between the columns.\n",
    "sns.heatmap(corr, cmap=\"coolwarm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's now create some Boxplots to detect the variability within each cluster.**\n",
    "\n",
    "Clients_0: 'non-high consumption'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating boxplot\n",
    "plt.subplots(figsize=(15, 8))\n",
    "bp = clients_0.drop(['CUPs'],axis=1).boxplot(column=list(clients_0.drop(['CUPs'],axis=1).columns))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clients_1: 'high consumption'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(15, 8))\n",
    "bp = clients_1.drop(['CUPs'],axis=1).boxplot(column=list(clients_1.drop(['CUPs'],axis=1).columns))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering\n",
    "Create some new features that may be interesting to reduce the dimensionality of the problem and improve the performance of the algorithm. New features starting from the hourly consumption (mean, max, std, mean(13h-21h))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b> Create some new features that may be interesting to reduce the dimensionality of the problem and improve the performance of the algorithm. \"max\" and \"min\" </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hours = list(consumption.drop(['CUPs', 'cluster'], axis=1))\n",
    "\n",
    "# Basic examples (please note that some of these characteristics may have a high correlation between them)\n",
    "consumption['average'] = consumption[hours].mean(axis=1)\n",
    "consumption['max'] = ## your code here\n",
    "consumption['min'] = ## your code here\n",
    "consumption['std'] = consumption[hours].std(axis=1)\n",
    "\n",
    "# Example minmax\n",
    "minmax = []\n",
    "# iteramos fila a fila en nuestro df\n",
    "for index, row in consumption.iterrows():\n",
    "    # si el mínimo es 0, fijaremos minmax a 0, para evitar una indeterminación 0/0\n",
    "    if row['min'] == 0:\n",
    "        minmax.append(0)\n",
    "    else:\n",
    "        minmax.append(row['min']/row['max'])\n",
    "consumption['minmax'] = minmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example average over a period of time. We have seen that between 13h and 21h there is a greater difference between clusters. \n",
    "peak_hours = ['h-' + str(x) for x in range(13,21)]\n",
    "consumption['peak_hours'] = consumption[peak_hours].mean(axis=1)\n",
    "\n",
    "consumption.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check the correlation matrix again**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(10, 8))\n",
    "corr = consumption.drop(['CUPs'],axis=1).corr()\n",
    "sns.heatmap(corr, cmap=\"coolwarm\")\n",
    "\n",
    "# Negative correlation (close to -1) is also interesting, as may be the case for minmax and cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data\n",
    "\n",
    "The seed ***randome_state=0*** is used for all exercises. ***Suffle=True*** indicates that the data is randomly split between training and test. This reduces the variance and prevents the model from overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = consumption.drop(['cluster'], axis=1) \n",
    "y = consumption['cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_size = 0.2  # percentage of the input data that I will use to validate the model\n",
    "random_state=0\n",
    "# Divide the data into training, validation and test data.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state,\n",
    "                                                    shuffle=True)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=test_size, random_state=random_state,\n",
    "                                                    shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b> Add more Classification algorithms </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "num_folds = 15\n",
    "error_metrics = {'balanced_accuracy'}\n",
    "models = {('LR', LogisticRegression()),\n",
    "           ('RF', RandomForestClassifier())}\n",
    "\n",
    "results = [] # stores the results of the evaluation metrics\n",
    "names = [] # name of each algorithm\n",
    "msg = [] # print the summary of the cross-validation method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Entreno con validación cruzada\n",
    "for scoring in error_metrics:\n",
    "    print('Classification evaluation metric: ', scoring)\n",
    "    for name, model in models:\n",
    "        print('Model ', name)\n",
    "        cross_validation = StratifiedShuffleSplit(n_splits=num_folds, random_state=0)\n",
    "        cv_results = cross_val_score(model, X_train, y_train, cv=cross_validation, scoring=scoring)\n",
    "        results.append(cv_results)\n",
    "        names.append(name)\n",
    "        resume = (name, cv_results.mean(), cv_results.std())\n",
    "        msg.append(resume)\n",
    "    print(msg)\n",
    "\n",
    "    # Comparar resultados entre algoritmos\n",
    "    fig = plt.figure()\n",
    "    fig.suptitle('Comparison of algorithms with evaluation metrics: %s' %scoring)\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_xlabel('Candidate models')\n",
    "    ax.set_ylabel('%s' %scoring)\n",
    "    plt.boxplot(results)\n",
    "    ax.set_xticklabels(names)\n",
    "    plt.show()\n",
    "\n",
    "    results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Hyperparameter setting*.\n",
    "\n",
    "Steps to perform hyperadjustment of parameters:\n",
    "\n",
    "* Metric to optimize: *balanced_accuracy*\n",
    "* Define search parameter ranges: *params*\n",
    "* Assign a validation method: *StratifiedShuffleSplit* (n_splits = 10).\n",
    "* Train with the validation data: *X_val*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "params = {\n",
    "     'n_estimators': [100, 600], #default=100\n",
    "     'min_samples_split': [2,5] #default=2\n",
    " }\n",
    "scoring='balanced_accuracy'\n",
    "cross_validation = StratifiedShuffleSplit(n_splits=10, random_state=0)\n",
    "my_cv = cross_validation.split(X_val, y_val)\n",
    "gsearch = GridSearchCV(estimator=model, param_grid=params, scoring=scoring, cv=my_cv)\n",
    "gsearch.fit(X_val, y_val)\n",
    "\n",
    "print(\"Best results: %f using the following hyperparameters %s\" % (gsearch.best_score_, gsearch.best_params_))\n",
    "means = gsearch.cv_results_['mean_test_score']\n",
    "stds = gsearch.cv_results_['std_test_score']\n",
    "params = gsearch.cv_results_['params']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final evaluation of the model.\n",
    "Evaluation metrics:\n",
    "  * 1. Confusion matrix\n",
    "  * 2. Matthews Coefficient (MCC)\n",
    "  * 3. ROC / AUC curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf_model = RandomForestClassifier(max_features='auto', min_samples_split=2,  n_estimators=600)\n",
    "clf_model.fit(X_train,y_train)  # The RF model is trained\n",
    "y_predict = clf_model.predict(X_test)  # Predictions are calculated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Print the feature ranking importance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Get the feature importance from the best RF model\n",
    "attribute_importance = gsearch.best_estimator_.feature_importances_\n",
    "feature_names = X.columns.tolist()\n",
    "\n",
    "\n",
    "# Create a DataFrame for feature importances\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': attribute_importance})\n",
    "\n",
    "# Sort the DataFrame by importance\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plotting the feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance_df, palette='rocket')\n",
    "plt.title('Feature Importances from Random Forest Model')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()\n",
    "\n",
    "# Print the most relevant features\n",
    "print(\"Most Relevant Features:\")\n",
    "print(feature_importance_df.head(10))  # Show top 10 features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Confusion matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "confusion_matrix = confusion_matrix(y_test, y_predict)\n",
    "print(classification_report(y_test, y_predict))\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "# clf_model is your trained classifier, and X_test, y_test are the test data and labels\n",
    "disp = ConfusionMatrixDisplay.from_estimator(\n",
    "    clf_model, X_test, y_test,\n",
    "    cmap=plt.cm.Blues\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Matthews Coefficient (MCC)**.\n",
    "\n",
    "The MCC uses correlation coefficients between -1 and +1. \n",
    "* Coefficient +1 represents a perfect prediction.\n",
    "* Coefficient 0 represents a random mean prediction.\n",
    "* Coefficient -1 represents an inverse prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "matthews_corrcoef(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. ROC curve / AUC**.\n",
    "\n",
    "* ROC curve: Curve of the true positive rate versus false positive rate at different classification thresholds.\n",
    "\n",
    "* AUC: (Area under the curve): The area under the curve (AUC) ROC is the probability that a classifier is more confident that a randomly chosen positive example is truly positive relative to a randomly chosen negative example being positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, RocCurveDisplay, roc_curve\n",
    "\n",
    "\n",
    "# Plot ROC curve using RocCurveDisplay\n",
    "RocCurveDisplay.from_estimator(clf_model, X_test, y_test)\n",
    "plt.show()\n",
    "\n",
    "# Calculate AUC score\n",
    "# Use predicted probabilities for the positive class\n",
    "y_prob = clf_model.predict_proba(X_test)[:, 1]\n",
    "auc_score = roc_auc_score(y_test, y_prob)\n",
    "print('AUC: %.3f' % auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

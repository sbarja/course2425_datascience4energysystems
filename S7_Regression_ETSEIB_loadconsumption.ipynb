{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b46813f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Exercise: Day-Ahead Consumption Prediction for the ETSEIB Building\n",
    "\n",
    "\n",
    "In this exercise, the goal is to train a machine learning regression model to predict the next day’s energy consumption for the ETSEIB building at UPC.\n",
    "\n",
    "The [SIRENA](https://upcsirena.app.dexma.com/analysis/consumption/display.htm) tool (Information System for Energy and Water Resources of UPC) is used for monitoring and evaluating energy and resource usage across UPC. It tracks the consumption of electricity, gas, and water, along with photovoltaic energy production and indoor air quality across UPC facilities.\n",
    "\n",
    "\n",
    "<img src=\"Figures/sirena-upc.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "\n",
    "\n",
    "* **Objective**\n",
    "Develop a model that forecasts the ETSEIB building’s energy consumption (in kWh) for the following day. The model will rely on historical consumption data and meteorological information to make accurate predictions.\n",
    "\n",
    "* **Data Sources**\n",
    "    * Consumption Data: Historical data on electricity, gas, and water consumption can be downloaded from the SIRENA platform, which centralizes utility usage data for UPC.\n",
    "\n",
    "    * Weather Data: Meteorological data is available from Spain’s Open Data service, AEMET. This data includes relevant weather information, which can be a key factor in predicting energy consumption patterns.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0cdc9a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div style=\"background-color: #ffffe0; padding: 15px; border-radius: 5px;\">\n",
    "\n",
    "# **Let's build a first model!**\n",
    "\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd16d52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import libraries\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)  # Suppress FutureWarnings\n",
    "\n",
    "\n",
    "# We load the input data set\n",
    "dataset = pd.read_excel('Data/etseib-consumption.xlsx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f613ab",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f0f0f0; padding: 25px; border-radius: 5px; font-weight: bold; margin-top: 25px;\">\n",
    "\n",
    "## 1. Understanding the Data\n",
    "    \n",
    "</div>\n",
    "\n",
    "> It is necessary to visualize and understand the data we are going to work with, as well as to know its characteristics.\n",
    "> \n",
    "> - How many rows do we have? How many attributes are there in the data?\n",
    "> - What are these attributes?\n",
    "> - Is there any missing data?\n",
    "> - Statistical summary of the input data set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2732542",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dataset shape\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabdbf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First 5 rows\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f390cb8c",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"background-color: #f0f0f0; padding: 25px; border-radius: 5px; margin-top: 25px;\">\n",
    "\n",
    "Let's delete the repeated column \n",
    "    \n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1192b7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.drop(columns=[\"Hour\"], inplace=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75454f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data format\n",
    "dataset.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc8c813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing data\n",
    "dataset.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf296d25",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f0f0f0; padding: 25px; border-radius: 5px; margin-top: 25px;\">\n",
    "\n",
    "Plot the ETSEIB dataset consumption\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480850f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set Seaborn style for better aesthetics\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create a larger figure for better clarity\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "sns.lineplot(data=dataset, x='Datetime', y='ETSEIB_consumption', color='royalblue', linewidth=0.75)\n",
    "\n",
    "# Title and labels\n",
    "plt.title('ETSEIB Consumption', fontsize=16, weight='bold')\n",
    "plt.xlabel('Time (Hours)', fontsize=14, labelpad=10)\n",
    "plt.ylabel('Consumption (kWh)', fontsize=14, labelpad=10)\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "plt.xticks(rotation=0, fontsize=12)\n",
    "\n",
    "# Increase y-axis label font size for better visibility\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "# Show gridlines for better readability of the plot\n",
    "plt.grid(True)\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b56fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900c185b",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f0f0f0; padding: 25px; border-radius: 5px; margin-top: 25px;\">\n",
    "\n",
    "Create the ProfileReport() \n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0a350a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_profiling import ProfileReport\n",
    "##pandas profiling\n",
    "#apply ProfileReport\n",
    "profile = ProfileReport(dataset, title='Profile Report')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a187d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile.to_file(\"your_report2.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1400bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e47ee5a2",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f0f0f0; padding: 25px; border-radius: 5px; margin-top: 25px;\">\n",
    "\n",
    "We have noticed that there is a row that is repeated. We delete it    \n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9dd2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate rows\n",
    "dataset = dataset.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf740c3",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"background-color: #f0f0f0; padding: 25px; border-radius: 5px; font-weight: bold; margin-top: 25px;\">\n",
    "\n",
    "## 2. Exploratory Data Analisys (EDA). Visualize the data. \n",
    "    \n",
    "</div>\n",
    "\n",
    "> Exploratory Data Analysis is the process of analyzing and visualizing datasets to summarize their main characteristics, with the help of graphical representations. It is a critical step in the machine learning pipeline because it helps to understand the structure, patterns, and relationships within the data before applying machine learning models.\n",
    "\n",
    "> The **goal** of EDA is to gain insights into the data, identify any issues such as missing values or outliers, and make informed decisions about data preprocessing, feature engineering, and model selection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f5de9a",
   "metadata": {},
   "source": [
    "> ### Histogram,  density plots and boxplots\n",
    "\n",
    "> * **Histograms** are useful when you need to see the actual count of data points in each range and when you want to visualize the data's distribution with specific bins.\n",
    "\n",
    "> * **Density** plots are better when you want to visualize the overall shape of the data's distribution and avoid the randomness introduced by binning in histograms. Density plots are also great for comparing distributions between groups or variables because they provide a smooth estimate without the arbitrary choice of bin size.\n",
    "\n",
    "> * A **boxplot** is a graphical representation of the distribution of a dataset, highlighting its median, interquartile range (IQR), and outliers. In machine learning, boxplots are useful for detecting outliers, understanding data distribution, and guiding feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f4983f",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f0f0f0; padding: 25px; border-radius: 5px;font-weight: bold; margin-top: 25px;\">\n",
    "\n",
    "Histogram\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0151091",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(dataset[\"ETSEIB_consumption\"], kde=True, bins=100, color='skyblue')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdcf207",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f0f0f0; padding: 25px; border-radius: 5px;font-weight: bold; margin-top: 25px;\">\n",
    "\n",
    "Density plots\n",
    "    \n",
    "</div>\n",
    "\n",
    "> * Let's compare the distributions for each month and compare them with the overall dataset\n",
    "> * In order to do so, we need to create a new input feature: **month**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1aaf996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new input feature: month\n",
    "dataset['month'] = dataset['Datetime'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784f8b77",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plotting Density Plot for 12 Months\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(data=dataset, x='ETSEIB_consumption', fill=True, color='gray', alpha=0.3, label='Overall Distribution')\n",
    "\n",
    "# Loop through each month\n",
    "for month in range(1, 13):\n",
    "    sns.kdeplot(data=dataset[dataset['month'] == month], x='ETSEIB_consumption', \n",
    "                fill=False, common_norm=False, label=month)\n",
    "    \n",
    "plt.title('Density Plot for Consumption Over 12 Months')\n",
    "plt.xlabel('Consumption')\n",
    "plt.ylabel('Density')\n",
    "plt.legend(title='Month')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddaca2e3",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"background-color: #f0f0f0; padding: 25px; border-radius: 5px;  margin-top: 25px;\">\n",
    "Plot the monthly distributions separetly    \n",
    "</div>\n",
    "\n",
    "> If we want to see more clearly the distribution of each month, we can also plot them separately. It can be seen that for the month of August, the distribution is atypical compared to the rest of the months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9991d0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Set up the figure with 12 subplots (3 rows x 4 columns)\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16, 10), sharex=True, sharey=True)\n",
    "fig.suptitle('Density Plot of ETSEIB Consumption for Each Month', fontsize=16)\n",
    "\n",
    "# Loop through each month and create a density plot in each subplot\n",
    "for month in range(1, 13):\n",
    "    ax = axes[(month-1) // 4, (month-1) % 4]  # Determine subplot position\n",
    "    sns.kdeplot(\n",
    "        data=dataset[dataset['month'] == month], \n",
    "        x='ETSEIB_consumption', \n",
    "        fill=True, \n",
    "        common_norm=False, \n",
    "        ax=ax\n",
    "    )\n",
    "    ax.set_title(f'Month: {month}')  # Title each subplot with the month\n",
    "    ax.set_xlabel('')  # Remove x-label to keep it clean\n",
    "    ax.set_ylabel('')  # Remove y-label to keep it clean\n",
    "\n",
    "# Set common labels\n",
    "fig.text(0.5, 0.04, 'Consumption', ha='center', fontsize=12)\n",
    "fig.text(0.04, 0.5, 'Density', va='center', rotation='vertical', fontsize=12)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust layout to make space for titles\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcd39e4",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f0f0f0; padding: 25px; border-radius: 5px;font-weight: bold; margin-top: 25px;\">\n",
    "\n",
    "Boxplot\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cad78e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "atributos_boxplot = dataset.plot(kind='box', subplots=True, figsize=(15, 10), sharex=False,\n",
    "                                 sharey=False, fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14a136b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "<div style=\"background-color: #f0f0f0; padding: 25px; border-radius: 5px;font-weight: bold; margin-top: 25px;\">\n",
    "\n",
    "#### Can we create more new meaninful features? \n",
    "    \n",
    "</div>\n",
    "\n",
    "> In order to analyze the consumption behavior of the ETSEIB, we will add new input variables to plot the different distributions of the data by month, to see the differences in consumption between weekdays and weekends, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4fb3d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Extract Hour, Day of Week\n",
    "dataset.loc[:,'day_of_week'] = dataset['Datetime'].dt.weekday\n",
    "dataset.loc[:,'hour'] = dataset['Datetime'].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d38a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc24455",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f0f0f0; padding: 25px; border-radius: 5px; font-weight: bold; margin-top: 25px;\">\n",
    "\n",
    "Plotting Weekend vs Weekdays Comparison\n",
    "</div>\n",
    "\n",
    "> Assuming that weekdays are Monday (0) to Friday (4) and weekends are Saturday (5) and Sunday (6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2c93e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset.loc[:,'is_weekend'] = dataset['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "\n",
    "# Group the data by 'is_weekend' and plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='is_weekend', y='ETSEIB_consumption', data=dataset, palette='Set2')\n",
    "plt.title('Weekend vs Weekday Consumption')\n",
    "plt.xlabel('Weekend (1) vs Weekday (0)')\n",
    "plt.ylabel('Consumption')\n",
    "plt.xticks([0, 1], ['Weekday', 'Weekend'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79ad7b6",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f0f0f0; padding: 25px; border-radius: 5px; margin-top: 25px;\">\n",
    "\n",
    "Plot distribution of data for weekdays and weekends\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dd4f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Plotting Weekend vs Weekdays Comparison (Density Plot)\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.kdeplot(data=dataset, x='ETSEIB_consumption', fill=True, color='gray', alpha=0.3, label='Overall Distribution')\n",
    "\n",
    "# Plot weekday consumption distribution\n",
    "sns.kdeplot(data=dataset[dataset['is_weekend'] == 0], x='ETSEIB_consumption', \n",
    "            fill=False, color='blue', label='Weekday', linewidth = 0.75)\n",
    "\n",
    "# Plot weekend consumption distribution\n",
    "sns.kdeplot(data=dataset[dataset['is_weekend'] == 1], x='ETSEIB_consumption', \n",
    "            fill=False, color='red', label='Weekend', linewidth = 0.75)\n",
    "\n",
    "# Title and labels\n",
    "plt.title('Density Plot for Consumption: Weekday vs Weekend', fontsize=16)\n",
    "plt.xlabel('Consumption', fontsize=14)\n",
    "plt.ylabel('Density', fontsize=14)\n",
    "plt.legend(title='Distribution', fontsize=12)  # Adding a legend\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98c2641",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f0f0f0; padding: 25px; border-radius: 5px;font-weight: bold; margin-top: 25px;\">\n",
    "\n",
    "DETECT outliers\n",
    "    \n",
    "</div>\n",
    "\n",
    "> * First, we will DETECT when they occur.\n",
    "> * They will not be removed for now, as we will use lag variables in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c955ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "threshold = 3   # Mark as outlier if Z-score > threshold\n",
    "\n",
    "dataset_outliers = dataset.copy()\n",
    "\n",
    "# Detect outliers using Z-score\n",
    "dataset_outliers['z_score'] = stats.zscore(dataset['ETSEIB_consumption'])\n",
    "dataset_outliers['outlier'] = np.abs(dataset_outliers['z_score']) > threshold \n",
    "dataset_outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549b1103",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"background-color: #f0f0f0; padding: 25px; border-radius: 5px; font-weight: bold; margin-top: 25px;\">\n",
    "\n",
    "When do the outliers occur? \n",
    "</div>\n",
    "\n",
    "\n",
    "> Month vs Weekday\n",
    "\n",
    "> Month vs Hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da80a38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Month vs Weekday\n",
    "outlier_counts = dataset_outliers[dataset_outliers['outlier']].groupby(['day_of_week', 'month']).size().unstack(fill_value=0)\n",
    "\n",
    "# Plot heatmap of outlier counts\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(outlier_counts, annot=True, cmap='YlOrRd', cbar_kws={'label': 'Outlier Count'})\n",
    "plt.title('Outlier Counts by Weekday and Month')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Weekday')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f78f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Month vs Hour\n",
    "outlier_counts = dataset_outliers[dataset_outliers['outlier']].groupby(['hour', 'month']).size().unstack(fill_value=0)\n",
    "\n",
    "# Plot heatmap of outlier counts\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(outlier_counts, annot=True, cmap='YlOrRd', cbar_kws={'label': 'Outlier Count'})\n",
    "plt.title('Outlier Counts by Weekday and Month')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('hour')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347ce08a",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f0f0f0; padding: 25px; border-radius: 5px;font-weight: bold; margin-top: 25px;\">\n",
    "\n",
    "Correlation matrix\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5e3a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Seaborn visualization library\n",
    "import seaborn as sns\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "corr = dataset.iloc[:,1:].corr(method='pearson') \n",
    "\n",
    "# Plot Heat Map,\n",
    "f, ax = plt.subplots(figsize=(12, 10))\n",
    "sns.heatmap(corr, annot=True, fmt=\".2f\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1220154d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "<div style=\"background-color: #f0f0f0; padding: 25px; border-radius: 5px; font-weight: bold; margin-top: 25px;\">\n",
    "\n",
    "## 3. Data Preparation\n",
    "    \n",
    "</div>\n",
    "\n",
    "> * Data cleaning\n",
    "> * Feature selection (create/delete/select)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4e5cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply one-hot encoding to the 'hour' column and day_of_the_week\n",
    "dataset = pd.get_dummies(dataset, columns=['day_of_week'], prefix='day_of_week',  dtype=float)\n",
    "dataset = pd.get_dummies(dataset, columns=['hour'], prefix='hour',  dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185e55b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5eae410",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<div style=\"background-color: #f0f0f0; padding: 25px; border-radius: 5px;font-weight: bold; margin-top: 25px;\">\n",
    "\n",
    "Create lag variables. Why are they important?\n",
    "    \n",
    "</div>\n",
    "\n",
    "> * Capturing Temporal Dependence: Many datasets, especially time series data, exhibit autocorrelation—where past values of a variable influence its future values. By incorporating lag variables, regression models can account for this temporal relationship.\n",
    "> * Improving Forecasting: In time series forecasting, lag variables help to make predictions based on prior values. This is particularly important for variables that follow cyclical patterns or trends over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d43005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lag variables for the previous day and two days ago\n",
    "dataset['Lag_1_day'] = dataset['ETSEIB_consumption'].shift(24)  # Lag of 24 hours (1 day)\n",
    "dataset['Lag_2_days'] = dataset['ETSEIB_consumption'].shift(48)  # Lag of 48 hours (2 days)\n",
    "dataset['Lag_7_days'] = dataset['ETSEIB_consumption'].shift(168)  # Lag of 48 hours (2 days)\n",
    "dataset['Lag_14_days'] = dataset['ETSEIB_consumption'].shift(336)  # Lag of 48 hours (2 days)\n",
    "dataset['Lag_21_days'] = dataset['ETSEIB_consumption'].shift(504)  # Lag of 48 hours (2 days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b8d4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fa50df",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27eb8ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d96eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the datetime column as index\n",
    "dataset_v1 = dataset.set_index('Datetime')\n",
    "dataset_v1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4ab91a",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"background-color: #f0f0f0; padding: 25px; border-radius: 5px;font-weight: bold; margin-top: 25px;\">\n",
    "\n",
    "Plot the correlation matrix again\n",
    "    \n",
    "</div>\n",
    "\n",
    "> A correlation matrix is a table that shows the pairwise correlation coefficients between a set of variables (or features) in a dataset. Each element in the matrix represents the correlation between two features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b52f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Seaborn visualization library\n",
    "import seaborn as sns\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "corr = dataset_v1.corr(method='pearson') \n",
    "\n",
    "# Plot Heat Map,\n",
    "f, ax = plt.subplots(figsize=(28, 20))\n",
    "sns.heatmap(corr, annot=True, fmt=\".2f\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eee85d5",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "<div style=\"background-color: #f0f0f0; padding: 25px; border-radius: 5px; font-weight: bold; margin-top: 25px;\">\n",
    "\n",
    "## 4. Split the data\n",
    "    \n",
    "</div>\n",
    "\n",
    "> * Divide the data into attributes: X (features) and tags: y (target).\n",
    "> * Scale the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830992d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features X ; Target y \n",
    "X = dataset_v1.drop(['ETSEIB_consumption'], axis=1) \n",
    "y = dataset_v1['ETSEIB_consumption']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e85818",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f0f0f0; padding: 25px; border-radius: 5px; margin-top: 25px;\">\n",
    "\n",
    "\n",
    "The data are divided into training data ``X_train``, ``y_train``, validation data ``X_val``, ``y_val`` and test data ``X_test``, ``y_test``.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21ec027",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_size = 0.15  # percentage of the input data that I will use to validate the model\n",
    "\n",
    "# I divide the data into training, validation and test data.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size,\n",
    "                                                    shuffle=False)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=test_size,\n",
    "                                                    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54496ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b49de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09831102",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "<div style=\"background-color: #f0f0f0; padding: 25px; border-radius: 5px; margin-top: 25px;\">\n",
    "\n",
    "Let's scale the dataset\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62112d50",
   "metadata": {},
   "source": [
    "The data is scaled using the ``MinMaxScaler()`` method, which scales and translates each attribute individually such that it is within the range [0, 1]. This needs to be done when the scales of the attributes are different (e.g. radiation [0, 650], wind speed [2, 15]).\n",
    "\n",
    "\n",
    "* ``MinMaxScaler()``: This scaler will normalize the values of the features to be within a specific range, typically [0, 1]. It does this by subtracting the minimum value and dividing by the range (max - min).\n",
    "* ``fit_transform(X_train)``: This step calculates the Min and Max values from the X_train data and applies the scaling transformation.\n",
    "* ``transform(X_val) and transform(X_test)``: These steps scale the validation and test sets using the same scaling parameters (Min and Max) derived from the training set, ensuring that data leakage doesn't occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bc2429",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the scaler on the training data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Use the same scaler to transform the validation and test data (do not fit again)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2facb79",
   "metadata": {},
   "source": [
    "\n",
    "Now, X_train_scaled, X_val_scaled, X_test_scaled are scaled versions of the original datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e60f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdb159e",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"background-color: #f0f0f0; padding: 25px; border-radius: 5px; font-weight: bold; margin-top: 25px;\">\n",
    "\n",
    "## 6. Model building and evaluation\n",
    "    \n",
    "</div>\n",
    "\n",
    "> First, let's check the vailable [Scoring Metrics in ScikitLearn](https://scikit-learn.org/1.5/api/sklearn.metrics.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fc7726",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import get_scorer_names\n",
    "\n",
    "\n",
    "print(get_scorer_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b77b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "# Define the number of folds and error metrics\n",
    "num_folds = 5\n",
    "error_metrics = {'neg_root_mean_squared_error', 'r2'}\n",
    "\n",
    "\n",
    "# Define a dictionary with models\n",
    "models = {\n",
    "    ('MLP', MLPRegressor()),\n",
    "    ('RFR', RandomForestRegressor()),\n",
    "    ('SVR', SVR()),\n",
    "    ('AdaB', AdaBoostRegressor()),\n",
    "    ('GBR', GradientBoostingRegressor()),  # Gradient Boosting Regressor\n",
    "    ('DTR', DecisionTreeRegressor()),  # Decision Tree Regressor\n",
    "    ('XGB', XGBRegressor()),  # XGBoost Regressor\n",
    "    ('LR', LinearRegression()),  # Linear Regression\n",
    "    ('EN', ElasticNet())  # ElasticNet Regressor\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d35de6",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<div style=\"background-color: #f0f0f0; padding: 25px; border-radius: 5px; margin-top: 25px;\">\n",
    "\n",
    "\n",
    "\n",
    "Each of the models is trained, the results are saved and compared visually.\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad366f5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "# Suppress specific warnings from sklearn (like ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "\n",
    "# Cross-validation training\n",
    "for scoring in error_metrics:\n",
    "    results = [] # store metrics results\n",
    "    msg = []  # print summary of result\n",
    "    names = []  # store name of the models\n",
    "    print('####### Evaluation metric: ', scoring)\n",
    "    \n",
    "    for name, model in models:\n",
    "        print(f'\\nTraining model: {name} with {scoring}...')\n",
    "        cross_validation = TimeSeriesSplit(n_splits=num_folds)\n",
    "        \n",
    "        # Start the cross-validation process and print verbose output\n",
    "        print(f\"Performing TimeSeriesSplit with {num_folds} folds...\")\n",
    "        \n",
    "        cv_results = cross_val_score(model, X_train_scaled, y_train, cv=cross_validation, scoring=scoring)\n",
    "        \n",
    "        print(f\"Model: {name}, {scoring} Mean: {cv_results.mean():.4f}, Std: {cv_results.std():.4f}\\n\")\n",
    "\n",
    "        results.append(cv_results)\n",
    "        names.append(name)\n",
    "        resume = (name, cv_results.mean(), cv_results.std())\n",
    "        msg.append(resume)\n",
    "    \n",
    "\n",
    "    # Compare results between algorithms\n",
    "    fig = plt.figure()\n",
    "    fig.suptitle('Compare metric result for algorithms: %s' %scoring)\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_xlabel('Candidate models')\n",
    "    ax.set_ylabel('%s' %scoring)\n",
    "    plt.boxplot(results)\n",
    "    ax.set_xticklabels(names)\n",
    "    # Show a grid for better readability\n",
    "    ax.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "    results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b019791",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<div style=\"background-color: #f0f0f0; padding: 25px; border-radius: 5px; font-weight: bold; margin-top: 25px;\">\n",
    "\n",
    "## 7. Best Model Hyperparameters Adjustment\n",
    "    \n",
    "</div>\n",
    "\n",
    "> Steps to perform the hyperadjustment of the parameters:\n",
    "> * Specify the model to be adjusted\n",
    "> * Specify a metric to optimize\n",
    "> * Define the search hyperparameter ranges: *params*\n",
    "> * Assign a validation method: *KFold*\n",
    "> * Find the Hyperparameters with the validation data: *X_val*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b8b170",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor()\n",
    "scoring= 'r2'\n",
    "params = {\n",
    "    # Number of trees in random forest\n",
    "    'n_estimators': [100, 500],  # default=100\n",
    "     # Maximum number of levels in tree\n",
    "   #  'max_depth': [2, None],  #deafult = None\n",
    "     # Method of selecting samples for training each tree\n",
    "}\n",
    "\n",
    "\n",
    "# Search for the best combination of hyperparameters\n",
    "cross_validation = TimeSeriesSplit(n_splits=5)\n",
    "my_cv = cross_validation.split(X_val_scaled)\n",
    "gsearch = GridSearchCV(estimator=model, param_grid=params, scoring=scoring, cv=my_cv, verbose=3)\n",
    "gsearch.fit(X_val_scaled, y_val)\n",
    "\n",
    "# Print best Result\n",
    "print(\"Best result: %f using the following hyperparameters %s\" % (gsearch.best_score_, gsearch.best_params_))\n",
    "means = gsearch.cv_results_['mean_test_score']\n",
    "stds = gsearch.cv_results_['std_test_score']\n",
    "params = gsearch.cv_results_['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eddd5a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9fc1b231",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "<div style=\"background-color: #f0f0f0; padding: 25px; border-radius: 5px; font-weight: bold; margin-top: 25px;\">\n",
    "\n",
    "## 8. Final evaluation of the model\n",
    "    \n",
    "</div>\n",
    "\n",
    "> * Finally, the model is built.\n",
    "> *     \n",
    "The ``fit()`` model is trained with the optimal hyperparameters found in the previous section and then the predictions are made. \n",
    "> * Use the ``X_test`` data to make the predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3539dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model_v1 = RandomForestRegressor(n_estimators=500, max_depth=None) ## train again with the winner model from the Grid Search\n",
    "final_model_v1.fit(X_train_scaled,y_train)  # Model training \n",
    "y_predict_v1 = final_model_v1.predict(X_test_scaled)  # prediction calculation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bfea8f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<div style=\"background-color: #f0f0f0; padding: 25px; border-radius: 5px; margin-top: 25px;\">\n",
    "\n",
    "\n",
    "Calculate the Evaluation Metrics for this final model\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7d9343",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# Calculate R² (R-squared) score\n",
    "r2 = r2_score(y_test, y_predict_v1)\n",
    "\n",
    "# Calculate RMSE (Root Mean Squared Error)\n",
    "rmse = mean_squared_error(y_test, y_predict_v1, squared=False)\n",
    "\n",
    "# Print both the R² and RMSE scores\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef5656c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<div style=\"background-color: #f0f0f0; padding: 25px; border-radius: 5px; margin-top: 25px;\">\n",
    "\n",
    "\n",
    "Plot the predictions ``y_predict`` vs the real values ``y_test``\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23b00b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Create the figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add the trace for the actual consumption (True Values)\n",
    "fig.add_trace(go.Scatter(x=y_test.index, y=y_test, mode='lines', name='True Values',\n",
    "                         line=dict(color='blue', width=2)))\n",
    "\n",
    "# Add the trace for the predicted consumption (Predicted Values)\n",
    "fig.add_trace(go.Scatter(x=y_test.index, y=y_predict_v1, mode='lines', name='Predicted Values V1',\n",
    "                         line=dict(color='red', width=2, dash='dot')))  # 'dot' for less separated dashes\n",
    "\n",
    "\n",
    "# Update layout for a more beautiful plot\n",
    "fig.update_layout(\n",
    "    title='True vs Predicted ETSEIB Consumption',\n",
    "    xaxis_title='Date/Time',\n",
    "    yaxis_title='Consumption (kWh)',\n",
    "    template='plotly',  # dark theme, can change to 'plotly' for light theme\n",
    "    hovermode='x unified',  # hover over to show values for both lines at a time\n",
    "    legend=dict(\n",
    "        x=0.01, y=0.99,  # position of legend\n",
    "        traceorder='normal',\n",
    "        font=dict(family=\"Arial\", size=12, color=\"white\"),\n",
    "        bgcolor='rgba(0, 0, 0, 0.3)',\n",
    "        bordercolor='white',\n",
    "        borderwidth=1\n",
    "    ),\n",
    "   \n",
    "   \n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66454332",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "     <b>  </b>\n",
    "  \n",
    "## What happens to the model? Can we improve it? How? \n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57f0bbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e1cbd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f949429",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c175d14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e064710b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2aeb66f3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div style=\"background-color: #ffffe0; padding: 15px; border-radius: 5px;\">\n",
    "\n",
    "# **Let's build a SECOND model!** \n",
    "    \n",
    "    ``model_v2``\n",
    "\n",
    "</div>\n",
    "\n",
    "> * We will use the Holidays library to identify holidays in Catalonia.\n",
    "> * Thus, a new input feature will be created.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8787f5b5",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"background-color: #f0f0f0; padding: 25px; border-radius: 5px; margin-top: 25px;\">\n",
    "\n",
    "Use the library Holidays\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec8ba23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import holidays  # https://pypi.org/project/holidays/\n",
    "\n",
    "\n",
    "# Use the 'holidays' library to get public holidays in Spain for both 2023 and 2024\n",
    "es_holidays_2023 = holidays.Spain(years=2023, prov='CT')  # Catalonia\n",
    "es_holidays_2024 = holidays.Spain(years=2024, prov='CT')  # Catalonia\n",
    "\n",
    "# Combine both holidays (2023 and 2024) into one set\n",
    "all_holidays = {**es_holidays_2023, **es_holidays_2024}\n",
    "\n",
    "all_holidays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90359560",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"background-color: #f0f0f0; padding: 25px; border-radius: 5px; margin-top: 25px;\">\n",
    "\n",
    "Create a second version of the dataset  ``dataset_v2``\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d88401",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_v2 = dataset.copy()\n",
    "\n",
    "# Check if the Datetime column dates are in 'all_holidays'\n",
    "dataset_v2['is_holiday'] = dataset_v2[\"Datetime\"].dt.date.isin(all_holidays.keys()).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e385a924",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0dddd1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a860197b",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"background-color: #f0f0f0; padding: 25px; border-radius: 5px;font-weight: bold; margin-top: 25px;\">\n",
    "\n",
    "Plot the correlation matrix again\n",
    "    \n",
    "</div>\n",
    "\n",
    "> A correlation matrix is a table that shows the pairwise correlation coefficients between a set of variables (or features) in a dataset. Each element in the matrix represents the correlation between two features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02f2004",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Seaborn visualization library\n",
    "import seaborn as sns\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "corr = dataset_v2.iloc[:,1:].corr(method='pearson') \n",
    "\n",
    "# Plot Heat Map,\n",
    "f, ax = plt.subplots(figsize=(25, 20))\n",
    "sns.heatmap(corr, annot=True, fmt=\".2f\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa99210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the datetime column as index\n",
    "dataset_v2 = dataset_v2.set_index('Datetime')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c103459d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "<div style=\"background-color: #f0f0f0; padding: 25px; border-radius: 5px; font-weight: bold; margin-top: 25px;\">\n",
    "\n",
    "## 4. Split the data\n",
    "    \n",
    "</div>\n",
    "\n",
    "> * Divide the data into attributes: X (features) and tags: y (target).\n",
    "> * Scale the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5c16ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features X ; Target y \n",
    "X = dataset_v2.drop(['ETSEIB_consumption'], axis=1) \n",
    "y = dataset_v2['ETSEIB_consumption']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0980d1f",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f0f0f0; padding: 25px; border-radius: 5px; margin-top: 25px;\">\n",
    "\n",
    "\n",
    "The data are divided into training data ``X_train``, ``y_train``, validation data ``X_val``, ``y_val`` and test data ``X_test``, ``y_test``.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f379e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_size = 0.15  # percentage of the input data that I will use to validate the model\n",
    "\n",
    "# I divide the data into training, validation and test data.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size,\n",
    "                                                    shuffle=False)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=test_size,\n",
    "                                                    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558a43f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d0a7c0d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "<div style=\"background-color: #f0f0f0; padding: 25px; border-radius: 5px; margin-top: 25px;\">\n",
    "\n",
    "Let's scale the dataset\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f261710",
   "metadata": {},
   "source": [
    "The data is scaled using the ``MinMaxScaler()`` method, which scales and translates each attribute individually such that it is within the range [0, 1]. This needs to be done when the scales of the attributes are different (e.g. radiation [0, 650], wind speed [2, 15]).\n",
    "\n",
    "\n",
    "* ``MinMaxScaler()``: This scaler will normalize the values of the features to be within a specific range, typically [0, 1]. It does this by subtracting the minimum value and dividing by the range (max - min).\n",
    "* ``fit_transform(X_train)``: This step calculates the Min and Max values from the X_train data and applies the scaling transformation.\n",
    "* ``transform(X_val) and transform(X_test)``: These steps scale the validation and test sets using the same scaling parameters (Min and Max) derived from the training set, ensuring that data leakage doesn't occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9561116b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "scaler_v2 = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the scaler on the training data\n",
    "X_train_scaled = scaler_v2.fit_transform(X_train)\n",
    "\n",
    "# Use the same scaler to transform the validation and test data (do not fit again)\n",
    "X_val_scaled = scaler_v2.transform(X_val)\n",
    "X_test_scaled = scaler_v2.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ce455c",
   "metadata": {},
   "source": [
    "\n",
    "Now, X_train_scaled, X_val_scaled, X_test_scaled are scaled versions of the original datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750c8b3f",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"background-color: #f0f0f0; padding: 25px; border-radius: 5px; font-weight: bold; margin-top: 25px;\">\n",
    "\n",
    "## 6. Model building and evaluation\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7e0d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "# Define the number of folds and error metrics\n",
    "num_folds =3\n",
    "error_metrics = {'neg_root_mean_squared_error', 'r2'}\n",
    "\n",
    "\n",
    "# Define a dictionary with models\n",
    "models = {\n",
    "    ('MLP', MLPRegressor()),\n",
    "    ('RFR', RandomForestRegressor()),\n",
    "    ('SVR', SVR()),\n",
    "    ('AdaB', AdaBoostRegressor()),\n",
    "    ('GBR', GradientBoostingRegressor()),  # Gradient Boosting Regressor\n",
    "    ('DTR', DecisionTreeRegressor()),  # Decision Tree Regressor\n",
    "    ('XGB', XGBRegressor()),  # XGBoost Regressor\n",
    "    ('LR', LinearRegression()),  # Linear Regression\n",
    "    ('EN', ElasticNet())  # ElasticNet Regressor\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4d76fc",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<div style=\"background-color: #f0f0f0; padding: 25px; border-radius: 5px; margin-top: 25px;\">\n",
    "\n",
    "\n",
    "\n",
    "Each of the models is trained, the results are saved and compared visually.\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368b61b0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "# Suppress specific warnings from sklearn (like ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "\n",
    "# Cross-validation training\n",
    "for scoring in error_metrics:\n",
    "    results = [] # store metrics results\n",
    "    msg = []  # print summary of result\n",
    "    names = []  # store name of the models\n",
    "    print('####### Evaluation metric: ', scoring)\n",
    "    \n",
    "    for name, model in models:\n",
    "        print(f'\\nTraining model: {name} with {scoring}...')\n",
    "        cross_validation = TimeSeriesSplit(n_splits=num_folds)\n",
    "        \n",
    "        # Start the cross-validation process and print verbose output\n",
    "        print(f\"Performing TimeSeriesSplit with {num_folds} folds...\")\n",
    "        \n",
    "        cv_results = cross_val_score(model, X_train_scaled, y_train, cv=cross_validation, scoring=scoring)\n",
    "        \n",
    "        print(f\"Model: {name}, {scoring} Mean: {cv_results.mean():.4f}, Std: {cv_results.std():.4f}\\n\")\n",
    "\n",
    "        results.append(cv_results)\n",
    "        names.append(name)\n",
    "        resume = (name, cv_results.mean(), cv_results.std())\n",
    "        msg.append(resume)\n",
    "    \n",
    "\n",
    "    # Compare results between algorithms\n",
    "    fig = plt.figure()\n",
    "    fig.suptitle('Compare metric result for algorithms: %s' %scoring)\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_xlabel('Candidate models')\n",
    "    ax.set_ylabel('%s' %scoring)\n",
    "    plt.boxplot(results)\n",
    "    ax.set_xticklabels(names)\n",
    "    # Show a grid for better readability\n",
    "    ax.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "    results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d261580",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<div style=\"background-color: #f0f0f0; padding: 25px; border-radius: 5px; font-weight: bold; margin-top: 25px;\">\n",
    "\n",
    "## 7. Best Model Hyperparameters Adjustment\n",
    "    \n",
    "</div>\n",
    "\n",
    "> Steps to perform the hyperadjustment of the parameters:\n",
    "> * Specify the model to be adjusted\n",
    "> * Specify a metric to optimize\n",
    "> * Define the search hyperparameter ranges: *params*\n",
    "> * Assign a validation method: *KFold*\n",
    "> * Find the Hyperparameters with the validation data: *X_val*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2428fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor()\n",
    "scoring='r2'\n",
    "params = {\n",
    "    # Number of trees in random forest\n",
    "    'n_estimators': [100, 500],  # default=100\n",
    "     # Maximum number of levels in tree\n",
    "    'max_depth': [2, None],  #deafult = None\n",
    "     # Method of selecting samples for training each tree\n",
    "}\n",
    "\n",
    "\n",
    "# Search for the best combination of hyperparameters\n",
    "cross_validation = TimeSeriesSplit(n_splits=5)\n",
    "my_cv = cross_validation.split(X_val_scaled)\n",
    "gsearch = GridSearchCV(estimator=model, param_grid=params, scoring=scoring, cv=my_cv, verbose=3)\n",
    "gsearch.fit(X_val_scaled, y_val)\n",
    "\n",
    "# Print best Result\n",
    "print(\"Best result: %f using the following hyperparameters %s\" % (gsearch.best_score_, gsearch.best_params_))\n",
    "means = gsearch.cv_results_['mean_test_score']\n",
    "stds = gsearch.cv_results_['std_test_score']\n",
    "params = gsearch.cv_results_['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8e02d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ceeac722",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "<div style=\"background-color: #f0f0f0; padding: 25px; border-radius: 5px; font-weight: bold; margin-top: 25px;\">\n",
    "\n",
    "## 8. Final evaluation of the model\n",
    "    \n",
    "</div>\n",
    "\n",
    "> * Finally, the model is built.\n",
    "> *     \n",
    "The ``fit()`` model is trained with the optimal hyperparameters found in the previous section and then the predictions are made. \n",
    "> * Use the ``X_test`` data to make the predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3e6e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model_v2 = RandomForestRegressor(n_estimators=500, max_depth=None) ## train again with the winner model from the Grid Search\n",
    "final_model_v2.fit(X_train_scaled,y_train)  # Model training \n",
    "y_predict_v2 = final_model_v2.predict(X_test_scaled)  # prediction calculation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db372a89",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<div style=\"background-color: #f0f0f0; padding: 25px; border-radius: 5px; margin-top: 25px;\">\n",
    "\n",
    "\n",
    "Calculate the Evaluation Metrics for this final model\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6127318",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# Calculate R² (R-squared) score\n",
    "r2 = r2_score(y_test, y_predict_v2)\n",
    "\n",
    "# Calculate RMSE (Root Mean Squared Error)\n",
    "rmse = mean_squared_error(y_test, y_predict_v2, squared=False)\n",
    "\n",
    "# Print both the R² and RMSE scores\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecff8b2d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<div style=\"background-color: #f0f0f0; padding: 25px; border-radius: 5px; margin-top: 25px;\">\n",
    "\n",
    "\n",
    "Plot the predictions ``y_predict`` vs the real values ``y_test``\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63d00bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Create the figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add the trace for the actual consumption (True Values)\n",
    "fig.add_trace(go.Scatter(x=y_test.index, y=y_test, mode='lines', name='True Values',\n",
    "                         line=dict(color='blue', width=1.5)))\n",
    "\n",
    "# Add the trace for the predicted consumption (Predicted Values)\n",
    "fig.add_trace(go.Scatter(x=y_test.index, y=y_predict_v1, mode='lines', name='Predicted Values V1',\n",
    "                         line=dict(color='red', width=1.5, dash='dot')))  # 'dot' for less separated dashes\n",
    "\n",
    "\n",
    "# Add the trace for the predicted consumption (Predicted Values)\n",
    "fig.add_trace(go.Scatter(x=y_test.index, y=y_predict_v2, mode='lines', name='Predicted Values V2',\n",
    "                         line=dict(color='green', width=1.5, dash='dot')))  # 'dot' for less separated dashes\n",
    "\n",
    "\n",
    "# Update layout for a more beautiful plot\n",
    "fig.update_layout(\n",
    "    title='True vs Predicted ETSEIB Consumption',\n",
    "    xaxis_title='Date/Time',\n",
    "    yaxis_title='Consumption (kWh)',\n",
    "    template='plotly',  # dark theme, can change to 'plotly' for light theme\n",
    "    hovermode='x unified',  # hover over to show values for both lines at a time\n",
    "    legend=dict(\n",
    "        x=0.01, y=0.99,  # position of legend\n",
    "        traceorder='normal',\n",
    "        font=dict(family=\"Arial\", size=12, color=\"white\"),\n",
    "        bgcolor='rgba(0, 0, 0, 0.3)',\n",
    "        bordercolor='white',\n",
    "        borderwidth=1\n",
    "    ),\n",
    "   \n",
    "   \n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a7bdbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370910bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81fc157",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "295a159b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div style=\"background-color: #ffffe0; padding: 15px; border-radius: 5px;\">\n",
    "\n",
    "# **Let's build a THIRD model!** \n",
    "    \n",
    "    ``model_v3``\n",
    "\n",
    "</div>\n",
    "\n",
    "> * We will use the Holidays library to identify holidays in Catalonia.\n",
    "> * Thus, a new input feature will be created.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170b292e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "     <b>  </b>\n",
    "  \n",
    "\n",
    "### Let's keep improving\n",
    "\n",
    "* Let's build a 3th version of our model. \n",
    "* Para acortar los procesos, asumiremos que el mejor modelo es RF, y no testearemos todos los disponibles.\n",
    "    \n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99d67a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# As a starting point...\n",
    "dataset_v3 = dataset_v2.copy()\n",
    "dataset_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd395f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98184f6f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "<div style=\"background-color: #f0f0f0; padding: 25px; border-radius: 5px; font-weight: bold; margin-top: 25px;\">\n",
    "\n",
    "## 4. Split the data\n",
    "    \n",
    "</div>\n",
    "\n",
    "> * Divide the data into attributes: X (features) and tags: y (target).\n",
    "> * Scale the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2de57b",
   "metadata": {},
   "source": [
    "### Split the data into Attributes and Target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525927b1",
   "metadata": {},
   "source": [
    "I divide the data into **attributes**: X (features) and **tags**: y (target)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7676bd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features X ; Target y \n",
    "X = dataset_v3.drop(['ETSEIB_consumption'], axis=1) \n",
    "y = dataset_v3['ETSEIB_consumption']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fb3138",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f0f0f0; padding: 25px; border-radius: 5px; margin-top: 25px;\">\n",
    "\n",
    "\n",
    "The data are divided into training data ``X_train``, ``y_train``, validation data ``X_val``, ``y_val`` and test data ``X_test``, ``y_test``.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a8c280",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_size = 0.15  # percentage of the input data that I will use to validate the model\n",
    "\n",
    "# I divide the data into training, validation and test data.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size,\n",
    "                                                    shuffle=False)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=test_size,\n",
    "                                                    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f276d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f212f90",
   "metadata": {},
   "source": [
    "Let's scale the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe7a533",
   "metadata": {},
   "source": [
    "The data is scaled using the ``MinMaxScaler()`` method, which scales and translates each attribute individually such that it is within the range [0, 1]. This needs to be done when the scales of the attributes are different (e.g. radiation [0, 650], wind speed [2, 15]).\n",
    "\n",
    "\n",
    "* ``MinMaxScaler()``: This scaler will normalize the values of the features to be within a specific range, typically [0, 1]. It does this by subtracting the minimum value and dividing by the range (max - min).\n",
    "* ``fit_transform(X_train)``: This step calculates the Min and Max values from the X_train data and applies the scaling transformation.\n",
    "* ``transform(X_val) and transform(X_test)``: These steps scale the validation and test sets using the same scaling parameters (Min and Max) derived from the training set, ensuring that data leakage doesn't occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffabcd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "scaler_v3 = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the scaler on the training data\n",
    "X_train_scaled = scaler_v3.fit_transform(X_train)\n",
    "\n",
    "# Use the same scaler to transform the validation and test data (do not fit again)\n",
    "X_val_scaled = scaler_v3.transform(X_val)\n",
    "X_test_scaled = scaler_v3.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8181c879",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6eea1585",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"background-color: #f0f0f0; padding: 25px; border-radius: 5px; font-weight: bold; margin-top: 25px;\">\n",
    "\n",
    "Recursive Featuring Elimination\n",
    "    \n",
    "</div>\n",
    "\n",
    "> * Recursive Feature Engineering (often referred to as Recursive Feature Elimination, or RFE) in scikit-learn is a method used to select important features for a machine learning model by recursively removing less important ones. It works by training the model multiple times, each time eliminating the least important feature(s) based on model performance, until the desired number of features is reached.\n",
    "> * <img src=\"Figures/wrapper-method.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cafc953",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "\n",
    "# Initialize Recursive Feature Elimination with the model and specify the number of features to select\n",
    "n_features_to_select = 37  # Adjust as needed\n",
    "\n",
    "# Initialize the model\n",
    "model_rfe = RandomForestRegressor(n_estimators=500, random_state=42)\n",
    "rfe = RFE(estimator=model_rfe, n_features_to_select=n_features_to_select, verbose=3)\n",
    "\n",
    "# Fit RFE on the training data\n",
    "rfe.fit(X_train_scaled, y_train) \n",
    "\n",
    "\n",
    "# Get the mask of selected features\n",
    "selected_features_mask = rfe.support_\n",
    "selected_features = X.columns[selected_features_mask]  # List the selected feature names\n",
    "deleted_features = X.columns[~selected_features_mask]\n",
    "\n",
    "print(\"Selected features:\", selected_features)\n",
    "print(\"Deleted features:\", deleted_features)\n",
    "\n",
    "# Filter the original DataFrame to include only the selected features\n",
    "X_selected = X[selected_features]\n",
    "X_selected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49025e0f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<div style=\"background-color: #f0f0f0; padding: 25px; border-radius: 5px; margin-top: 25px;\">\n",
    "\n",
    "Print the selected and deleted features\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6219131",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "selected_features = X.columns[selected_features_mask]  # List the selected feature names\n",
    "deleted_features = X.columns[~selected_features_mask]\n",
    "\n",
    "print(\"Selected features:\", selected_features)\n",
    "print(\"Deleted features:\", deleted_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9971b7fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7d313d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Transform the train, validation, and test sets to include only the selected features\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_size = 0.15  # percentage of the input data that I will use to validate the model\n",
    "\n",
    "# I divide the data into training, validation and test data.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=test_size,\n",
    "                                                    shuffle=False)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=test_size,\n",
    "                                                    shuffle=False)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the scaler on the training data\n",
    "X_train_scaled_selected = scaler.fit_transform(X_train)\n",
    "\n",
    "# Use the same scaler to transform the validation and test data (do not fit again)\n",
    "X_val_scaled_selected = scaler.transform(X_val)\n",
    "X_test_scaled_selected = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a254347",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# selected_features_25 = ['month', 'is_weekend', 'day_of_week_0', 'day_of_week_1',\n",
    "#        'day_of_week_2', 'day_of_week_3', 'day_of_week_4', 'day_of_week_5',\n",
    "#        'hour_7', 'hour_8', 'hour_9', 'hour_10', 'hour_12', 'hour_13',\n",
    "#        'hour_14', 'hour_15', 'hour_16', 'hour_17', 'hour_18', 'Lag_1_day',\n",
    "#        'Lag_2_days', 'Lag_7_days', 'Lag_14_days', 'Lag_21_days', 'is_holiday']\n",
    "\n",
    "\n",
    "# selected_features_37 = ['month', 'is_weekend', 'day_of_week_0', 'day_of_week_1',\n",
    "#        'day_of_week_2', 'day_of_week_3', 'day_of_week_4', 'day_of_week_5',\n",
    "#        'day_of_week_6', 'hour_0', 'hour_1', 'hour_4', 'hour_5', 'hour_6',\n",
    "#        'hour_7', 'hour_8', 'hour_9', 'hour_10', 'hour_11', 'hour_12',\n",
    "#        'hour_13', 'hour_14', 'hour_15', 'hour_16', 'hour_17', 'hour_18',\n",
    "#        'hour_19', 'hour_20', 'hour_21', 'hour_22', 'hour_23', 'Lag_1_day',\n",
    "#        'Lag_2_days', 'Lag_7_days', 'Lag_14_days', 'Lag_21_days', 'is_holiday']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c59757",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"background-color: #f0f0f0; padding: 25px; border-radius: 5px; font-weight: bold; margin-top: 25px;\">\n",
    "\n",
    "## 6. Model building and evaluation\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244aeea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "# Define the number of folds and error metrics\n",
    "num_folds = 5\n",
    "error_metrics = {'neg_root_mean_squared_error', 'r2'}\n",
    "\n",
    "\n",
    "# Define a dictionary with models\n",
    "models = {\n",
    "    ('MLP', MLPRegressor()),\n",
    "    ('RFR', RandomForestRegressor()),\n",
    "    ('AdaB', AdaBoostRegressor()),\n",
    "    ('GBR', GradientBoostingRegressor()),  # Gradient Boosting Regressor\n",
    "    ('XGB', XGBRegressor()),  # XGBoost Regressor\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503ed094",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<div style=\"background-color: #f0f0f0; padding: 25px; border-radius: 5px; margin-top: 25px;\">\n",
    "\n",
    "\n",
    "\n",
    "Each of the models is trained, the results are saved and compared visually.\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26cb835",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "# Suppress specific warnings from sklearn (like ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "\n",
    "# Cross-validation training\n",
    "for scoring in error_metrics:\n",
    "    results = [] # store metrics results\n",
    "    msg = []  # print summary of result\n",
    "    names = []  # store name of the models\n",
    "    print('####### Evaluation metric: ', scoring)\n",
    "    \n",
    "    for name, model in models:\n",
    "        print(f'\\nTraining model: {name} with {scoring}...')\n",
    "        cross_validation = TimeSeriesSplit(n_splits=num_folds)\n",
    "        \n",
    "        # Start the cross-validation process and print verbose output\n",
    "        print(f\"Performing TimeSeriesSplit with {num_folds} folds...\")\n",
    "        \n",
    "        cv_results = cross_val_score(model, X_train_scaled_selected, y_train, cv=cross_validation, scoring=scoring)\n",
    "        \n",
    "        print(f\"Model: {name}, {scoring} Mean: {cv_results.mean():.4f}, Std: {cv_results.std():.4f}\\n\")\n",
    "\n",
    "        results.append(cv_results)\n",
    "        names.append(name)\n",
    "        resume = (name, cv_results.mean(), cv_results.std())\n",
    "        msg.append(resume)\n",
    "    \n",
    "\n",
    "    # Compare results between algorithms\n",
    "    fig = plt.figure()\n",
    "    fig.suptitle('Compare metric result for algorithms: %s' %scoring)\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_xlabel('Candidate models')\n",
    "    ax.set_ylabel('%s' %scoring)\n",
    "    plt.boxplot(results)\n",
    "    ax.set_xticklabels(names)\n",
    "    # Show a grid for better readability\n",
    "    ax.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "    results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5ccb78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50f23fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceac50d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2ca3012",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<div style=\"background-color: #f0f0f0; padding: 25px; border-radius: 5px; font-weight: bold; margin-top: 25px;\">\n",
    "\n",
    "## 7. Best Model Hyperparameters Adjustment\n",
    "    \n",
    "</div>\n",
    "\n",
    "> Steps to perform the hyperadjustment of the parameters:\n",
    "> * Specify the model to be adjusted\n",
    "> * Specify a metric to optimize\n",
    "> * Define the search hyperparameter ranges: *params*\n",
    "> * Assign a validation method: *KFold*\n",
    "> * Find the Hyperparameters with the validation data: *X_val*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4be9005",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83889228",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = RandomForestRegressor()\n",
    "scoring='r2'\n",
    "params = {\n",
    "    # Number of trees in random forest\n",
    "    'n_estimators': [100, 500],  # default=100\n",
    "     # Maximum number of levels in tree\n",
    "    'max_depth': [2, None],  #deafult = None\n",
    "     # Method of selecting samples for training each tree\n",
    "}\n",
    "\n",
    "\n",
    "# Search for the best combination of hyperparameters\n",
    "cross_validation = TimeSeriesSplit(n_splits=5)\n",
    "my_cv = cross_validation.split(X_val_scaled_selected)\n",
    "gsearch = GridSearchCV(estimator=model, param_grid=params, scoring=scoring, cv=my_cv, verbose=3)\n",
    "gsearch.fit(X_val_scaled_selected, y_val)\n",
    "\n",
    "# Print best Result\n",
    "print(\"Best result: %f using the following hyperparameters %s\" % (gsearch.best_score_, gsearch.best_params_))\n",
    "means = gsearch.cv_results_['mean_test_score']\n",
    "stds = gsearch.cv_results_['std_test_score']\n",
    "params = gsearch.cv_results_['params']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb0f6d9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "<div style=\"background-color: #f0f0f0; padding: 25px; border-radius: 5px; font-weight: bold; margin-top: 25px;\">\n",
    "\n",
    "## 8. Final evaluation of the model\n",
    "    \n",
    "</div>\n",
    "\n",
    "> * Finally, the model is built.\n",
    "> *     \n",
    "The ``fit()`` model is trained with the optimal hyperparameters found in the previous section and then the predictions are made. \n",
    "> * Use the ``X_test`` data to make the predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928d5390",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model_v3 = RandomForestRegressor(n_estimators=500, max_depth=None) ## train again with the winner model from the Grid Search\n",
    "final_model_v3.fit(X_train_scaled_selected,y_train)  # Model training \n",
    "y_predict_v3 = final_model_v3.predict(X_test_scaled_selected)  # prediction calculation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7d00e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# Calculate R² (R-squared) score\n",
    "r2 = r2_score(y_test, y_predict_v3)\n",
    "\n",
    "# Calculate RMSE (Root Mean Squared Error)\n",
    "rmse = mean_squared_error(y_test, y_predict_v3, squared=False)\n",
    "\n",
    "# Print both the R² and RMSE scores\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be118614",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Create the figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add the trace for the actual consumption (True Values)\n",
    "fig.add_trace(go.Scatter(x=y_test.index, y=y_test, mode='lines', name='True Values',\n",
    "                         line=dict(color='blue', width=2)))\n",
    "\n",
    "# Add the trace for the predicted consumption (Predicted Values)\n",
    "fig.add_trace(go.Scatter(x=y_test.index, y=y_predict_v1, mode='lines', name='Predicted Values V1',\n",
    "                         line=dict(color='orange', width=1, dash='dot')))  # 'dot' for less separated dashes\n",
    "\n",
    "# Add the trace for the predicted consumption (Predicted Values)\n",
    "fig.add_trace(go.Scatter(x=y_test.index, y=y_predict_v2, mode='lines', name='Predicted Values V2',\n",
    "                         line=dict(color='green', width=1, dash='dot')))  # 'dot' for less separated dashes\n",
    "\n",
    "# Add the trace for the predicted consumption (Predicted Values)\n",
    "fig.add_trace(go.Scatter(x=y_test.index, y=y_predict_v3, mode='lines', name='Predicted Values V3',\n",
    "                         line=dict(color='red', width=1, dash='dot')))  # 'dot' for less separated dashes\n",
    "\n",
    "\n",
    "# Update layout for a more beautiful plot\n",
    "fig.update_layout(\n",
    "    title='True vs Predicted ETSEIB Consumption',\n",
    "    xaxis_title='Date/Time',\n",
    "    yaxis_title='Consumption (kWh)',\n",
    "    template='plotly',  # dark theme, can change to 'plotly' for light theme\n",
    "    hovermode='x unified',  # hover over to show values for both lines at a time\n",
    "    legend=dict(\n",
    "        x=0.01, y=0.99,  # position of legend\n",
    "        traceorder='normal',\n",
    "        font=dict(family=\"Arial\", size=12, color=\"white\"),\n",
    "        bgcolor='rgba(0, 0, 0, 0.3)',\n",
    "        bordercolor='white',\n",
    "        borderwidth=1\n",
    "    ),\n",
    "   \n",
    "   \n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea771fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate R^2 and RMSE for each prediction version\n",
    "metrics = {\n",
    "    \"Model\": [\"Predicted Values V1\", \"Predicted Values V2\", \"Predicted Values V3\"],\n",
    "    \"R^2\": [\n",
    "        r2_score(y_test, y_predict_v1),\n",
    "        r2_score(y_test, y_predict_v2),\n",
    "        r2_score(y_test, y_predict_v3)\n",
    "    ],\n",
    "    \"RMSE\": [\n",
    "        np.sqrt(mean_squared_error(y_test, y_predict_v1)),\n",
    "        np.sqrt(mean_squared_error(y_test, y_predict_v2)),\n",
    "        np.sqrt(mean_squared_error(y_test, y_predict_v3))\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create the table with R^2 and RMSE values\n",
    "table_fig = go.Figure(data=[go.Table(\n",
    "    header=dict(values=[\"Model\", \"R^2\", \"RMSE\"],\n",
    "                fill_color='lightgrey',\n",
    "                align='center'),\n",
    "    cells=dict(values=[metrics[\"Model\"], metrics[\"R^2\"], metrics[\"RMSE\"]],\n",
    "               format=[\"\", \".3f\", \".3f\"],  # formatting for R^2 and RMSE\n",
    "               align='center')\n",
    ")])\n",
    "\n",
    "# Update layout for the table\n",
    "table_fig.update_layout(title=\"R^2 and RMSE for Predicted Models V1, V2, V3\")\n",
    "\n",
    "# Show both plots\n",
    "table_fig.show()   # Show the metrics table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49373009",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div style=\"background-color: #ffffe0; padding: 15px; border-radius: 5px;\">\n",
    "\n",
    "# **Store the model for production use**  \n",
    "    \n",
    "\n",
    "</div>\n",
    "\n",
    "> * The **joblib library** in scikit-learn is a powerful tool for efficiently saving, loading, and handling large Python objects, particularly when working with machine learning models and large datasets. It is designed to optimize performance for models that require a lot of memory or computation time to train. Here’s how it’s commonly used:\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12131ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(final_model_v2, 'Data/etseib_modelv2.joblib')\n",
    "\n",
    "# Save the scaler used for scaling the training data\n",
    "joblib.dump(scaler_v2, 'Data/scaler_v2.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62bce7a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<div style=\"background-color: #f0f0f0; padding: 25px; border-radius: 5px; margin-top: 25px;\">\n",
    "\n",
    "The model is now saved for future use. When we need to make new predictions, we will load the model and provide it with the input data.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a9710b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = joblib.load('Data/etseib_modelv2.joblib') # Load the ML model\n",
    "scaler_v2_loaded = joblib.load('Data/scaler_v2.joblib') # Load the saved scaler\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b749e91",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "input_data = pd.read_excel(\"Data/input_etseib_model_v2_.xlsx\")\n",
    "# Set the 'date_column' as the index, since it is not a feature in our model\n",
    "input_data.set_index('Datetime', inplace=True)\n",
    "input_data.head(24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86b7ca2",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"background-color: #f0f0f0; padding: 25px; border-radius: 5px; margin-top: 25px;\">\n",
    "\n",
    "The input data was scaled when the model was trained, so we need to scale new input data in the same way. We should use the scaler from the training data (``scaler_v2``) to ensure consistency.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e68d49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_scaled = scaler_v2_loaded.transform(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b916b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = loaded_model.predict(input_data_scaled)\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40200968",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_etseib_consumption = [131, 129, 125, 126, 123, 128, 194, 263, 346, 373, 426, 431, 426,\n",
    "       409, 380, 394, 382, 369, 323, 286, 225, 167, 141, 135]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5a0360",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Create the figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add the trace for the actual consumption (True Values)\n",
    "fig.add_trace(go.Scatter(x=input_data.index, y=real_etseib_consumption, mode='lines', name='True Values',\n",
    "                         line=dict(color='blue', width=2)))\n",
    "\n",
    "# Add the trace for the predicted consumption (Predicted Values)\n",
    "fig.add_trace(go.Scatter(x=input_data.index, y=y_predict, mode='lines', name='Predicted Values V1',\n",
    "                         line=dict(color='red', width=2, dash='dot')))  # 'dot' for less separated dashes\n",
    "\n",
    "\n",
    "\n",
    "# Update layout for a more beautiful plot\n",
    "fig.update_layout(\n",
    "    title='True vs Predicted ETSEIB Consumption for 6/11/2024. Model v2',\n",
    "    xaxis_title='Date/Time',\n",
    "    yaxis_title='Consumption (kWh)',\n",
    "    yaxis=dict(range=[0, max(max(real_etseib_consumption), max(y_predict)) * 1.1]),\n",
    "    template='plotly',  # dark theme, can change to 'plotly' for light theme\n",
    "    hovermode='x unified',  # hover over to show values for both lines at a time\n",
    "    legend=dict(\n",
    "        x=0.01, y=0.01,  # position of legend\n",
    "        traceorder='normal',\n",
    "        font=dict(family=\"Arial\", size=12, color=\"white\"),\n",
    "        bgcolor='rgba(0, 0, 0, 0.3)',\n",
    "        bordercolor='white',\n",
    "        borderwidth=1\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee614b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
